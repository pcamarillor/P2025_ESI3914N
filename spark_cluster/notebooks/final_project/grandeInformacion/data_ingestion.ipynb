{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2718b337",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **Kafka Producer: Financial Transaction Generator** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "\n",
    "**Team members**: \n",
    "- Miguel Alberto Torres Dueñas\n",
    "- Juan Pablo Cortez Navarro\n",
    "- Luther Williams Sandria \n",
    "- Ferdinand Bierbaum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c8e960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#0be7b65b50a239d7ee8b621f3c329b25c5c4aadafbae5ac7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e808d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://f5db43ce3d38:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc4d173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"67261d1016d7:9093\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example-0\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example-1\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example-2\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example-3\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9101b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pairs_array: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- video_id: string (nullable = true)\n",
      " |-- watch_time_seconds: string (nullable = true)\n",
      " |-- resolution: string (nullable = true)\n",
      " |-- bitrate_kbps: string (nullable = true)\n",
      " |-- buffering_events: string (nullable = true)\n",
      " |-- paused: string (nullable = true)\n",
      " |-- skipped: string (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- recommended: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col, expr\n",
    "\n",
    "kafka_df = kafka_lines.select(split(col(\"value\"), \",\").alias(\"pairs_array\"))\n",
    "\n",
    "kafka_df = kafka_df.withColumn(\"user_id\", split(col(\"pairs_array\").getItem(0), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"video_id\", split(col(\"pairs_array\").getItem(1), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"watch_time_seconds\", split(col(\"pairs_array\").getItem(2), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"resolution\", split(col(\"pairs_array\").getItem(3), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"bitrate_kbps\", split(col(\"pairs_array\").getItem(4), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"buffering_events\", split(col(\"pairs_array\").getItem(5), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"paused\", split(col(\"pairs_array\").getItem(6), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"skipped\", split(col(\"pairs_array\").getItem(7), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"genre\", split(col(\"pairs_array\").getItem(8), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"region\", split(col(\"pairs_array\").getItem(9), \":\").getItem(1))\n",
    "kafka_df = kafka_df.withColumn(\"recommended\", split(col(\"pairs_array\").getItem(10), \":\").getItem(1))\n",
    "\n",
    "# Usamos expr para hacer la resta de longitud directamente\n",
    "kafka_df = kafka_df.withColumn(\n",
    "    \"timestamp\",\n",
    "    expr(\"substring(split(pairs_array[11], ':')[1], 1, length(split(pairs_array[11], ':')[1]) - 1)\")\n",
    ")\n",
    "\n",
    "kafka_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a17866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = kafka_df \\\n",
    "#                 .writeStream \\\n",
    "#                 .outputMode(\"append\") \\\n",
    "#                 .trigger(processingTime='3 seconds') \\\n",
    "#                 .format(\"console\") \\\n",
    "#                 .option(\"truncate\", \"false\") \\\n",
    "#                 .start()\n",
    "\n",
    "# query.awaitTermination(300)\n",
    "# query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8b97666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/12 00:13:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/12 00:13:35 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/12 00:16:24 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3005 milliseconds\n",
      "25/05/12 00:16:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3280 milliseconds\n",
      "25/05/12 00:16:39 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3092 milliseconds\n",
      "25/05/12 00:16:48 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3641 milliseconds\n",
      "25/05/12 00:17:00 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3691 milliseconds\n",
      "25/05/12 00:17:04 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3911 milliseconds\n",
      "25/05/12 00:17:08 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3482 milliseconds\n",
      "25/05/12 00:17:11 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3698 milliseconds\n",
      "25/05/12 00:17:15 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3265 milliseconds\n",
      "25/05/12 00:17:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3333 milliseconds\n",
      "25/05/12 00:17:25 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 4145 milliseconds\n",
      "25/05/12 00:17:28 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3217 milliseconds\n",
      "25/05/12 00:17:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3736 milliseconds\n",
      "25/05/12 00:17:36 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 4144 milliseconds\n",
      "25/05/12 00:17:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 3739 milliseconds\n",
      "25/05/12 00:17:44 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 3000 milliseconds, but spent 4247 milliseconds\n"
     ]
    }
   ],
   "source": [
    "query_files = kafka_df \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .trigger(processingTime='3 seconds') \\\n",
    "                .format(\"parquet\") \\\n",
    "                .option(\"path\", \"/home/jovyan/notebooks/data/project_parquet\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .option(\"checkpointLocation\", \"/home/jovyan/checkpoint\") \\\n",
    "                .start()\n",
    "query_files.awaitTermination(300)\n",
    "query_files.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62d10ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+------------------+----------+------------+----------------+------+-------+--------------+------+-----------+-------------+\n",
      "|pairs_array                                                                                                                                                                                                                                                                           |user_id     |video_id  |watch_time_seconds|resolution|bitrate_kbps|buffering_events|paused|skipped|genre         |region|recommended|timestamp    |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+------------------+----------+------------+----------------+------+-------+--------------+------+-----------+-------------+\n",
      "|[{\"user_id\": \"user_4137\",  \"video_id\": \"vid_034\",  \"watch_time_seconds\": 3469,  \"resolution\": \"720p\",  \"bitrate_kbps\": 1371,  \"buffering_events\": 2,  \"paused\": false,  \"skipped\": true,  \"genre\": \"Horror\",  \"region\": \"US\",  \"recommended\": true,  \"timestamp\": \"05/12/2025\"}]      | \"user_4137\"| \"vid_034\"| 3469             | \"720p\"   | 1371       | 2              | false| true  | \"Horror\"     | \"US\" | true      | \"05/12/2025\"|\n",
      "|[{\"user_id\": \"user_9570\",  \"video_id\": \"vid_013\",  \"watch_time_seconds\": 772,  \"resolution\": \"4K\",  \"bitrate_kbps\": 2470,  \"buffering_events\": 1,  \"paused\": true,  \"skipped\": true,  \"genre\": \"Sci-Fi\",  \"region\": \"JP\",  \"recommended\": true,  \"timestamp\": \"05/12/2025\"}]          | \"user_9570\"| \"vid_013\"| 772              | \"4K\"     | 2470       | 1              | true | true  | \"Sci-Fi\"     | \"JP\" | true      | \"05/12/2025\"|\n",
      "|[{\"user_id\": \"user_6444\",  \"video_id\": \"vid_076\",  \"watch_time_seconds\": 1558,  \"resolution\": \"480p\",  \"bitrate_kbps\": 1296,  \"buffering_events\": 1,  \"paused\": false,  \"skipped\": true,  \"genre\": \"Documentary\",  \"region\": \"US\",  \"recommended\": false,  \"timestamp\": \"05/12/2025\"}]| \"user_6444\"| \"vid_076\"| 1558             | \"480p\"   | 1296       | 1              | false| true  | \"Documentary\"| \"US\" | false     | \"05/12/2025\"|\n",
      "|[{\"user_id\": \"user_9572\",  \"video_id\": \"vid_078\",  \"watch_time_seconds\": 883,  \"resolution\": \"4K\",  \"bitrate_kbps\": 6426,  \"buffering_events\": 3,  \"paused\": true,  \"skipped\": true,  \"genre\": \"Comedy\",  \"region\": \"MX\",  \"recommended\": false,  \"timestamp\": \"05/12/2025\"}]         | \"user_9572\"| \"vid_078\"| 883              | \"4K\"     | 6426       | 3              | true | true  | \"Comedy\"     | \"MX\" | false     | \"05/12/2025\"|\n",
      "|[{\"user_id\": \"user_5070\",  \"video_id\": \"vid_087\",  \"watch_time_seconds\": 3103,  \"resolution\": \"480p\",  \"bitrate_kbps\": 1572,  \"buffering_events\": 2,  \"paused\": true,  \"skipped\": true,  \"genre\": \"Drama\",  \"region\": \"BR\",  \"recommended\": false,  \"timestamp\": \"05/12/2025\"}]       | \"user_5070\"| \"vid_087\"| 3103             | \"480p\"   | 1572       | 2              | true | true  | \"Drama\"      | \"BR\" | false     | \"05/12/2025\"|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+----------+------------------+----------+------------+----------------+------+-------+--------------+------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/notebooks/data/project_parquet\")\n",
    "df.show(5, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e7d5be",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Data type array<string> of column pairs_array is not supported.\nData type string of column user_id is not supported.\nData type string of column video_id is not supported.\nData type string of column watch_time_seconds is not supported.\nData type string of column resolution is not supported.\nData type string of column bitrate_kbps is not supported.\nData type string of column buffering_events is not supported.\nData type string of column paused is not supported.\nData type string of column skipped is not supported.\nData type string of column genre is not supported.\nData type string of column region is not supported.\nData type string of column recommended is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Crear vector de características\u001b[39;00m\n\u001b[1;32m      9\u001b[0m assembler \u001b[38;5;241m=\u001b[39m VectorAssembler(inputCols\u001b[38;5;241m=\u001b[39mfeature_cols, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m assembled_data \u001b[38;5;241m=\u001b[39m \u001b[43massembler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/ml/wrapper.py:398\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 398\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Data type array<string> of column pairs_array is not supported.\nData type string of column user_id is not supported.\nData type string of column video_id is not supported.\nData type string of column watch_time_seconds is not supported.\nData type string of column resolution is not supported.\nData type string of column bitrate_kbps is not supported.\nData type string of column buffering_events is not supported.\nData type string of column paused is not supported.\nData type string of column skipped is not supported.\nData type string of column genre is not supported.\nData type string of column region is not supported.\nData type string of column recommended is not supported."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5c6f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde535e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
