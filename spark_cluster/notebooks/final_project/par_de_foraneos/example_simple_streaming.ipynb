{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **Streamer Structrure for final Proyect** </center>\n",
    "### <center> **Par de Foraneos** </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from foraneos.utils_proyectofinal_copy import resample_and_aggregate\n",
    "from foraneos.utils_proyectofinal_copy import SparkUtils as SpU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-32bd4ffb-928b-40d1-813d-0384ee9a6b09;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 729ms :: artifacts dl 29ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-32bd4ffb-928b-40d1-813d-0384ee9a6b09\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/21ms)\n",
      "25/05/13 01:30:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '672e28abb623',\n",
    "                'Aaron' : 'a5ab6bdab4b3'}\n",
    "KAFKA_SERVER = {'Konrad': 'dee5c9cc3710:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .load()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ), \n",
    "                                              (\"open\", \"float\" ), \n",
    "                                              (\"high\", \"float\" ), \n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" ) ,\n",
    "                                              (\"williams_r\", \"float\" ),  \n",
    "                                              (\"rsi\", \"float\"), \n",
    "                                              (\"ultimate_osc\", \"float\"), \n",
    "                                              (\"ema\", \"float\"), \n",
    "                                              (\"close_lag_1\", \"float\" ), \n",
    "                                              (\"close_lag_2\", \"float\" ), \n",
    "                                              (\"close_lag_3\", \"float\" ), \n",
    "                                              (\"close_lag_4\", \"float\" ), \n",
    "                                              (\"close_lag_5\", \"float\" ),                                                                                 \n",
    "                                              ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "       \n",
    "           \n",
    "    df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "    df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "    df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "           .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "           .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "          .select(\"timestamp\", \"company\",\"close\")\n",
    "          \n",
    "    df.printSchema()\n",
    "    custom_resampler = resample_and_aggregate(new_window=5)\n",
    "    resampled_df = df.mapInPandas(custom_resampler, schema=result_schema)\n",
    "\n",
    "    streamer_df.append(resampled_df)\n",
    "    resampled_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarking to handle late arrival events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 01:30:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-f3538344-09fa-47ca-8ec5-c2be9fc9756b. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a8e313ab-bd72-4596-ae0e-98db97aeff48. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-932c2fab-e68f-4c89-8b09-bc9617413bb6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-93b264a8-53b8-4564-b044-d06aa2b7b153. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 01:30:49 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 01:30:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 01:30:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 01:30:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 01:30:49 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 01:32:06 WARN TaskSetManager: Lost task 0.0 in stage 4.0 (TID 4) (172.18.0.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:06 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 5) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:09 WARN TaskSetManager: Lost task 0.0 in stage 7.0 (TID 7) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:09 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 6) (172.18.0.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:19 WARN TaskSetManager: Lost task 0.3 in stage 6.0 (TID 15) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:19 ERROR TaskSetManager: Task 0 in stage 6.0 failed 4 times; aborting job\n",
      "25/05/13 01:32:19 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/05/13 01:32:19 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/05/13 01:32:19 ERROR MicroBatchExecution: Query [id = 90163c09-1f48-47a4-9ca8-16c9ff8c311d, runId = 282fc348-94e2-492e-9dbf-b31d84dd4dfd] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 15) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:333)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 01:32:21 WARN TaskSetManager: Lost task 0.3 in stage 5.0 (TID 17) (172.18.0.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:21 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job\n",
      "25/05/13 01:32:21 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/05/13 01:32:21 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/05/13 01:32:21 ERROR MicroBatchExecution: Query [id = 8015ae73-7d2c-4cf3-a1ce-9fc5fb700ea0, runId = 99874707-1183-4267-9860-e16b603bec92] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 17) (172.18.0.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:333)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 01:32:21 WARN TaskSetManager: Lost task 0.3 in stage 4.0 (TID 16) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:21 ERROR TaskSetManager: Task 0 in stage 4.0 failed 4 times; aborting job\n",
      "25/05/13 01:32:21 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/05/13 01:32:21 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/05/13 01:32:21 ERROR MicroBatchExecution: Query [id = 9bb1ffaa-3d3a-4826-92b8-32e348d0d457, runId = cf9ae637-6f44-4617-99b0-fcd41dc80e3f] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 16) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:333)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 01:32:24 WARN TaskSetManager: Lost task 0.2 in stage 7.0 (TID 18) (172.18.0.4 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "25/05/13 01:32:26 ERROR TaskSetManager: Task 0 in stage 7.0 failed 4 times; aborting job\n",
      "25/05/13 01:32:26 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] is aborting.\n",
      "25/05/13 01:32:26 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 1, writer: ConsoleWriter[numRows=20, truncate=true]] aborted.\n",
      "25/05/13 01:32:26 ERROR MicroBatchExecution: Query [id = fbde046d-4bf1-4456-9f95-f710ff772ca0, runId = 58e14fe4-8993-4ce3-9b44-2291a2de19dd] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7.0 (TID 19) (172.18.0.7 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:333)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:437)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:390)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:364)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:312)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3575)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 470, in dump_stream\n",
      "    return ArrowStreamSerializer.dump_stream(self, init_stream_yield_batches(), stream)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 100, in dump_stream\n",
      "    for batch in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/sql/pandas/serializers.py\", line 463, in init_stream_yield_batches\n",
      "    for series in iterator:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 931, in func\n",
      "    for result_batch, result_type in result_iter:\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 205, in verify_element\n",
      "    verify_pandas_result(\n",
      "  File \"/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/pyspark.zip/pyspark/worker.py\", line 252, in verify_pandas_result\n",
      "    raise PySparkRuntimeError(\n",
      "pyspark.errors.exceptions.base.PySparkRuntimeError: [RESULT_COLUMNS_MISMATCH_FOR_PANDAS_UDF] Column names of the returned pandas.DataFrame do not match specified schema. Missing: close_lag_5. Unexpected: ema.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:594)\n",
      "\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:576)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:446)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "\n",
    "# for i in range(4):\n",
    "#     query.append(\n",
    "#         streamer_df[i] \\\n",
    "#         .writeStream \\\n",
    "#         .outputMode(\"append\") \\\n",
    "#         .trigger(processingTime='120 seconds') \\\n",
    "#         .format(\"parquet\") \\\n",
    "#         .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "#         .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "#         .start()\n",
    "#     )\n",
    "\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='120 seconds') \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    #query[i].awaitTermination(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output0/part-00000-cc734dfc-75e1-436c-a412-6da58b98bef5-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 01:30:20 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/temporary-56d8ca74-e634-421b-8bc3-cd1920bd23d6. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/temporary-56d8ca74-e634-421b-8bc3-cd1920bd23d6\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 01:30:20 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/temporary-796cb787-8456-4430-af45-0241c4ad1f4c. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/temporary-796cb787-8456-4430-af45-0241c4ad1f4c\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
