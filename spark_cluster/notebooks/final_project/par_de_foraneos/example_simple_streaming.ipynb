{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **Streamer Structrure for final Proyect** </center>\n",
    "### <center> **Par de Foraneos** </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from foraneos.utils_proyectofinal_copy import resample_and_aggregate\n",
    "from foraneos.utils_proyectofinal_copy import SparkUtils as SpU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-66247010-7307-4a5b-9b94-d771dc4715cc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 835ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-66247010-7307-4a5b-9b94-d771dc4715cc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/17ms)\n",
      "25/05/13 07:24:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '672e28abb623',\n",
    "                'Aaron' : 'a5ab6bdab4b3'}\n",
    "KAFKA_SERVER = {'Konrad': 'dee5c9cc3710:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 07:25:29 ERROR Utils: Uncaught exception in thread driver-heartbeater\n",
      "java.lang.ClassFormatError: Incompatible magic value 0 in class file sun/reflect/misc/MethodUtil\n",
      "\tat java.management/com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:193)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.ConvertingMethod.invokeWithOpenReturn(ConvertingMethod.java:175)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:117)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanIntrospector.invokeM2(MXBeanIntrospector.java:54)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)\n",
      "\tat java.management/javax.management.StandardMBean.getAttribute(StandardMBean.java:372)\n",
      "\tat java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:644)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:679)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanProxy$GetHandler.invoke(MXBeanProxy.java:122)\n",
      "\tat java.management/com.sun.jmx.mbeanserver.MXBeanProxy.invoke(MXBeanProxy.java:167)\n",
      "\tat java.management/javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:258)\n",
      "\tat jdk.proxy1/jdk.proxy1.$Proxy25.getMemoryUsed(Unknown Source)\n",
      "\tat org.apache.spark.metrics.MBeanExecutorMetricType.getMetricValue(ExecutorMetricType.scala:67)\n",
      "\tat org.apache.spark.metrics.SingleValueExecutorMetricType.getMetricValues(ExecutorMetricType.scala:46)\n",
      "\tat org.apache.spark.metrics.SingleValueExecutorMetricType.getMetricValues$(ExecutorMetricType.scala:44)\n",
      "\tat org.apache.spark.metrics.MBeanExecutorMetricType.getMetricValues(ExecutorMetricType.scala:60)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1(ExecutorMetrics.scala:103)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.$anonfun$getCurrentMetrics$1$adapted(ExecutorMetrics.scala:102)\n",
      "\tat scala.collection.immutable.Vector.foreach(Vector.scala:1856)\n",
      "\tat org.apache.spark.executor.ExecutorMetrics$.getCurrentMetrics(ExecutorMetrics.scala:102)\n",
      "\tat org.apache.spark.SparkContext.reportHeartBeat(SparkContext.scala:2781)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$23(SparkContext.scala:592)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 07:25:30 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.6: Command exited with code 50\n",
      "25/05/13 07:25:30 ERROR Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.ClassFormatError: Incompatible magic value 1757185484 in class file java/util/concurrent/ConcurrentLinkedQueue$Itr\n",
      "\tat java.base/java.util.concurrent.ConcurrentLinkedQueue.iterator(ConcurrentLinkedQueue.java:729)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$JCollectionWrapper.iterator(JavaCollectionWrappers.scala:72)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:562)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:926)\n",
      "\tat org.apache.spark.scheduler.Pool.executorLost(Pool.scala:90)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:1146)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:1048)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$removeExecutor(CoarseGrainedSchedulerBackend.scala:472)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:223)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Exception in thread \"dispatcher-CoarseGrainedScheduler\" java.lang.ClassFormatError: Incompatible magic value 1757185484 in class file java/util/concurrent/ConcurrentLinkedQueue$Itr\n",
      "\tat java.base/java.util.concurrent.ConcurrentLinkedQueue.iterator(ConcurrentLinkedQueue.java:729)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$JCollectionWrapper.iterator(JavaCollectionWrappers.scala:72)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:562)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:926)\n",
      "\tat org.apache.spark.scheduler.Pool.executorLost(Pool.scala:90)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:1146)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:1048)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$removeExecutor(CoarseGrainedSchedulerBackend.scala:472)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:223)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 07:25:30 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.7: Command exited with code 50\n",
      "25/05/13 07:25:30 ERROR Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.ClassFormatError: Incompatible magic value 1757185484 in class file java/util/concurrent/ConcurrentLinkedQueue$Itr\n",
      "\tat java.base/java.util.concurrent.ConcurrentLinkedQueue.iterator(ConcurrentLinkedQueue.java:729)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$JCollectionWrapper.iterator(JavaCollectionWrappers.scala:72)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:562)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:926)\n",
      "\tat org.apache.spark.scheduler.Pool.executorLost(Pool.scala:90)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:1146)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:1048)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$removeExecutor(CoarseGrainedSchedulerBackend.scala:472)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:223)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Exception in thread \"dispatcher-CoarseGrainedScheduler\" java.lang.ClassFormatError: Incompatible magic value 1757185484 in class file java/util/concurrent/ConcurrentLinkedQueue$Itr\n",
      "\tat java.base/java.util.concurrent.ConcurrentLinkedQueue.iterator(ConcurrentLinkedQueue.java:729)\n",
      "\tat scala.collection.convert.JavaCollectionWrappers$JCollectionWrapper.iterator(JavaCollectionWrappers.scala:72)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:562)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:926)\n",
      "\tat org.apache.spark.scheduler.Pool.executorLost(Pool.scala:90)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.removeExecutor(TaskSchedulerImpl.scala:1146)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.executorLost(TaskSchedulerImpl.scala:1048)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.org$apache$spark$scheduler$cluster$CoarseGrainedSchedulerBackend$DriverEndpoint$$removeExecutor(CoarseGrainedSchedulerBackend.scala:472)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receive$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:223)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/05/13 07:25:30 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: FAILED\n",
      "25/05/13 07:25:30 ERROR Inbox: An error happened while processing message in the inbox for AppClient\n",
      "java.lang.NoClassDefFoundError: com/fasterxml/jackson/module/scala/ser/ScalaIteratorSerializerResolver$\n",
      "\tat com.fasterxml.jackson.module.scala.ser.IteratorSerializerModule.$init$(IteratorSerializerModule.scala:81)\n",
      "\tat com.fasterxml.jackson.module.scala.DefaultScalaModule.<init>(DefaultScalaModule.scala:18)\n",
      "\tat com.fasterxml.jackson.module.scala.DefaultScalaModule$.<init>(DefaultScalaModule.scala:36)\n",
      "\tat com.fasterxml.jackson.module.scala.DefaultScalaModule$.<clinit>(DefaultScalaModule.scala:36)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader$.<clinit>(ErrorClassesJSONReader.scala:92)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader.$anonfun$errorInfoMap$1(ErrorClassesJSONReader.scala:44)\n",
      "\tat scala.collection.immutable.List.map(List.scala:246)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader.<init>(ErrorClassesJSONReader.scala:44)\n",
      "\tat org.apache.spark.SparkThrowableHelper$.<clinit>(SparkThrowableHelper.scala:35)\n",
      "\tat org.apache.spark.SparkException.<init>(SparkException.scala:56)\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializerResolver$\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\t... 24 more\n",
      "Exception in thread \"dispatcher-event-loop-4\" java.lang.NoClassDefFoundError: com/fasterxml/jackson/module/scala/ser/ScalaIteratorSerializerResolver$\n",
      "\tat com.fasterxml.jackson.module.scala.ser.IteratorSerializerModule.$init$(IteratorSerializerModule.scala:81)\n",
      "\tat com.fasterxml.jackson.module.scala.DefaultScalaModule.<init>(DefaultScalaModule.scala:18)\n",
      "\tat com.fasterxml.jackson.module.scala.DefaultScalaModule$.<init>(DefaultScalaModule.scala:36)\n",
      "\tat com.fasterxml.jackson.module.scala.DefaultScalaModule$.<clinit>(DefaultScalaModule.scala:36)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader$.<clinit>(ErrorClassesJSONReader.scala:92)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader.$anonfun$errorInfoMap$1(ErrorClassesJSONReader.scala:44)\n",
      "\tat scala.collection.immutable.List.map(List.scala:246)\n",
      "\tat scala.collection.immutable.List.map(List.scala:79)\n",
      "\tat org.apache.spark.ErrorClassesJsonReader.<init>(ErrorClassesJSONReader.scala:44)\n",
      "\tat org.apache.spark.SparkThrowableHelper$.<clinit>(SparkThrowableHelper.scala:35)\n",
      "\tat org.apache.spark.SparkException.<init>(SparkException.scala:56)\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.module.scala.ser.ScalaIteratorSerializerResolver$\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\t... 24 more\n",
      "25/05/13 07:25:30 WARN ManagedSelector: \n",
      "java.lang.NoClassDefFoundError: org/sparkproject/jetty/util/IO\n",
      "\tat org.sparkproject.jetty.io.ManagedSelector$StopSelector.update(ManagedSelector.java:1048)\n",
      "\tat org.sparkproject.jetty.io.ManagedSelector$SelectorProducer.processUpdates(ManagedSelector.java:568)\n",
      "\tat org.sparkproject.jetty.io.ManagedSelector$SelectorProducer.produce(ManagedSelector.java:539)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produceTask(EatWhatYouKill.java:362)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:186)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
      "\tat org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:137)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
      "\tat org.sparkproject.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.ClassNotFoundException: org.sparkproject.jetty.util.IO\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\t... 10 more\n",
      "25/05/13 07:25:46 WARN StandaloneAppClient$ClientEndpoint: Connection to 672e28abb623:7077 failed; waiting for master to reconnect...\n",
      "25/05/13 07:25:46 WARN StandaloneSchedulerBackend: Disconnected from Spark cluster! Waiting for reconnection...\n",
      "25/05/13 07:25:46 WARN StandaloneAppClient$ClientEndpoint: Connection to 672e28abb623:7077 failed; waiting for master to reconnect...\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .load()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ), \n",
    "                                              (\"open\", \"float\" ), \n",
    "                                              (\"high\", \"float\" ), \n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" ) ,\n",
    "                                              (\"williams_r\", \"float\" ),  \n",
    "                                              (\"rsi\", \"float\"), \n",
    "                                              (\"ultimate_osc\", \"float\"), \n",
    "                                              (\"ema\", \"float\"), \n",
    "                                              (\"close_lag_1\", \"float\" ), \n",
    "                                              (\"close_lag_2\", \"float\" ), \n",
    "                                              (\"close_lag_3\", \"float\" ), \n",
    "                                              (\"close_lag_4\", \"float\" ), \n",
    "                                              (\"close_lag_5\", \"float\" ),                                                                                 \n",
    "                                              ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- ema: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- ema: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- ema: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- company: string (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- open: float (nullable = true)\n",
      " |-- high: float (nullable = true)\n",
      " |-- low: float (nullable = true)\n",
      " |-- close: float (nullable = true)\n",
      " |-- williams_r: float (nullable = true)\n",
      " |-- rsi: float (nullable = true)\n",
      " |-- ultimate_osc: float (nullable = true)\n",
      " |-- ema: float (nullable = true)\n",
      " |-- close_lag_1: float (nullable = true)\n",
      " |-- close_lag_2: float (nullable = true)\n",
      " |-- close_lag_3: float (nullable = true)\n",
      " |-- close_lag_4: float (nullable = true)\n",
      " |-- close_lag_5: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "             \n",
    "    df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "    df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "    df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "           .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "           .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "          .select(\"timestamp\", \"company\",\"close\")\n",
    "          \n",
    "    df.printSchema()\n",
    "    custom_resampler = resample_and_aggregate(new_window=0.5)\n",
    "    resampled_df = df.withWatermark(\"timestamp\", \"10 minutes\").groupBy(\"company\").applyInPandas(custom_resampler, schema=result_schema)\n",
    "\n",
    "    streamer_df.append(resampled_df)\n",
    "    resampled_df.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarking to handle late arrival events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 06:49:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9b4dd413-0c96-4da7-97e7-e5e25698f5bf. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-73abf87c-3d8c-4504-9a15-cddb2ceecc67. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-41acf780-7c43-4cc9-a9b5-8f82e49de05d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6ef7c844-b162-4657-9a2b-7a89d02c818d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/05/13 06:49:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 06:49:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 06:49:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 06:49:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/13 06:49:18 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "|timestamp|open|high|low|close|williams_r|rsi|ultimate_osc|ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "+---------+----+----+---+-----+----------+---+------------+---+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+--------+--------+--------+--------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|          timestamp|    open|    high|     low|   close|williams_r| rsi|ultimate_osc| ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+-------------------+--------+--------+--------+--------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|2025-05-13 06:49:30|210.8156|210.8156|210.8156|210.8156|      NULL|NULL|        NULL|NULL|       NULL|       NULL|       NULL|       NULL|       NULL|\n",
      "+-------------------+--------+--------+--------+--------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|          timestamp|     open|     high|      low|    close|williams_r| rsi|ultimate_osc| ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|2025-05-13 06:49:30|210.80597|210.80597|210.80597|210.80597|      NULL|NULL|        NULL|NULL|       NULL|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:00|210.81378|210.81378|210.71565|210.71565|      NULL|NULL|        NULL|NULL|  210.80597|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:30| 210.7415| 210.7415|210.66425| 210.7338|      NULL|NULL|        NULL|NULL|  210.71565|  210.80597|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:51:00|210.71445|210.77148|210.71088|210.77148|      NULL|NULL|        NULL|NULL|   210.7338|  210.71565|  210.80597|       NULL|       NULL|\n",
      "|2025-05-13 06:51:30|210.71602|210.72989|210.71094|210.72385|      NULL|NULL|        NULL|NULL|  210.77148|   210.7338|  210.71565|  210.80597|       NULL|\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|          timestamp|     open|     high|      low|    close|williams_r| rsi|ultimate_osc| ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|2025-05-13 06:49:30|141.66594|141.66841|141.66594|141.66841|      NULL|NULL|        NULL|NULL|       NULL|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:00|141.68523|141.71623|141.66225|141.71185|      NULL|NULL|        NULL|NULL|  141.66841|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:30|141.73235|141.76048|141.72438|141.72438|      NULL|NULL|        NULL|NULL|  141.71185|  141.66841|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:51:00|141.68704|141.73787|141.68704|141.69386|      NULL|NULL|        NULL|NULL|  141.72438|  141.71185|  141.66841|       NULL|       NULL|\n",
      "|2025-05-13 06:51:30|141.67145|141.71262|141.67145|141.71155|      NULL|NULL|        NULL|NULL|  141.69386|  141.72438|  141.71185|  141.66841|       NULL|\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+----------+----------+----------+----------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|          timestamp|      open|      high|       low|     close|williams_r| rsi|ultimate_osc| ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+-------------------+----------+----------+----------+----------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|2025-05-13 06:49:30|122.967094|122.967094|122.967094|122.967094|      NULL|NULL|        NULL|NULL|       NULL|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:00| 122.95711| 122.99625| 122.95711|122.967964|      NULL|NULL|        NULL|NULL| 122.967094|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:30|  122.9852|  122.9852| 122.94313| 122.96036|      NULL|NULL|        NULL|NULL| 122.967964| 122.967094|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:51:00| 122.96259| 122.99249| 122.96259| 122.99249|      NULL|NULL|        NULL|NULL|  122.96036| 122.967964| 122.967094|       NULL|       NULL|\n",
      "|2025-05-13 06:51:30|  122.9976|  123.0365|  122.9976| 123.02081|      NULL|NULL|        NULL|NULL|  122.99249|  122.96036| 122.967964| 122.967094|       NULL|\n",
      "+-------------------+----------+----------+----------+----------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|          timestamp|     open|     high|      low|    close|williams_r| rsi|ultimate_osc| ema|close_lag_1|close_lag_2|close_lag_3|close_lag_4|close_lag_5|\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "|2025-05-13 06:49:30|342.35825|342.37463|342.35825|342.37463|      NULL|NULL|        NULL|NULL|       NULL|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:00|342.42007|342.46634|342.34134|342.46634|      NULL|NULL|        NULL|NULL|  342.37463|       NULL|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:50:30|342.53864|342.73492|342.53864|342.73492|      NULL|NULL|        NULL|NULL|  342.46634|  342.37463|       NULL|       NULL|       NULL|\n",
      "|2025-05-13 06:51:00| 342.7472|342.81113|342.71878| 342.7527|      NULL|NULL|        NULL|NULL|  342.73492|  342.46634|  342.37463|       NULL|       NULL|\n",
      "|2025-05-13 06:51:30|342.76132|342.76132|342.62347|342.62347|      NULL|NULL|        NULL|NULL|   342.7527|  342.73492|  342.46634|  342.37463|       NULL|\n",
      "+-------------------+---------+---------+---------+---------+----------+----+------------+----+-----------+-----------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "\n",
    "# for i in range(4):\n",
    "#     query.append(\n",
    "#         streamer_df[i] \\\n",
    "#         .writeStream \\\n",
    "#         .outputMode(\"append\") \\\n",
    "#         .trigger(processingTime='120 seconds') \\\n",
    "#         .format(\"parquet\") \\\n",
    "#         .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "#         .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "#         .start()\n",
    "#     )\n",
    "\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='90 seconds') \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    #query[i].awaitTermination(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output0/part-00000-cc734dfc-75e1-436c-a412-6da58b98bef5-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
