{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **Streamer Structrure for final Proyect** </center>\n",
    "### <center> **Par de Foraneos** </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0038e828-9a61-422f-9e41-203e0ddd9e11;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 547ms :: artifacts dl 17ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0038e828-9a61-422f-9e41-203e0ddd9e11\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/10ms)\n",
      "25/05/13 21:16:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '2453c3db49e4',\n",
    "                'Aaron' : 'a5ab6bdab4b3'}\n",
    "KAFKA_SERVER = {'Konrad': '4c63f45c41b4:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"reader-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform binary data into string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarking to handle late arrival events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output0/*.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df0 = spark.read.parquet(files[0])\n",
    "for i in range(1,len(files)):\n",
    "    df1 = spark.read.parquet(files[i])\n",
    "    df0 = df0.union(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df0.orderBy(\"timestamp\", ascending=True).dropDuplicates([\"timestamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+---------+---------+---------+---------+\n",
      "|          timestamp|company|     open|     high|      low|    close|\n",
      "+-------------------+-------+---------+---------+---------+---------+\n",
      "|2025-05-13 20:15:00|    CAT|342.93753|343.20148|342.93753|343.20148|\n",
      "|2025-05-13 20:20:00|    CAT|345.62918|345.77954| 345.6173| 345.6173|\n",
      "|2025-05-13 20:25:00|    CAT| 345.5698|345.66568|345.42758| 345.5017|\n",
      "|2025-05-13 20:30:00|    CAT|343.89902|344.33096| 343.7308| 344.3125|\n",
      "|2025-05-13 20:35:00|    CAT|344.30045|344.30045|344.06238|344.28622|\n",
      "|2025-05-13 20:40:00|    CAT|344.26022| 344.7602|344.13525| 344.7602|\n",
      "|2025-05-13 20:45:00|    CAT|344.74054|345.38028|344.74054|345.35986|\n",
      "|2025-05-13 20:50:00|    CAT|345.40012|345.40012|344.73892|344.76483|\n",
      "|2025-05-13 20:55:00|    CAT| 344.6523|344.88815|344.62592| 344.8439|\n",
      "|2025-05-13 21:00:00|    CAT|344.83353|345.12054| 344.6902| 344.9121|\n",
      "|2025-05-13 21:05:00|    CAT|344.94507|344.94507|  344.495|344.85913|\n",
      "|2025-05-13 21:10:00|    CAT|344.80118| 345.2245| 344.6694|345.04062|\n",
      "|2025-05-13 21:15:00|    CAT|  345.314|345.41052|344.89026|344.96173|\n",
      "|2025-05-13 21:20:00|    CAT|344.99368|345.19568|344.86047| 345.1315|\n",
      "|2025-05-13 21:25:00|    CAT| 345.1402|345.22235| 345.0024|345.19177|\n",
      "|2025-05-13 21:30:00|    CAT|345.22336|345.79852|345.21582|345.79852|\n",
      "|2025-05-13 21:35:00|    CAT|345.71695|345.77383| 345.5635|345.58112|\n",
      "|2025-05-13 21:40:00|    CAT|345.59155|346.24304|345.59155| 346.0708|\n",
      "|2025-05-13 21:45:00|    CAT|346.03635| 346.1478|345.80142| 345.8252|\n",
      "|2025-05-13 21:50:00|    CAT| 345.8287| 345.8287|345.56335|345.62918|\n",
      "+-------------------+-------+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from foraneos.utils_proyectofinal_copy import calc_techincal_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating technical indicators...\n",
      "Creating lag feature for lag 1...\n",
      "Creating lag feature for lag 2...\n",
      "Creating lag feature for lag 3...\n",
      "Creating lag feature for lag 4...\n",
      "Creating lag feature for lag 5...\n",
      "            timestamp company        open        high         low       close  \\\n",
      "0 2025-05-13 20:15:00     CAT  342.937531  343.201477  342.937531  343.201477   \n",
      "1 2025-05-13 20:20:00     CAT  345.629181  345.779541  345.617310  345.617310   \n",
      "2 2025-05-13 20:25:00     CAT  345.569794  345.665680  345.427582  345.501709   \n",
      "3 2025-05-13 20:30:00     CAT  343.899017  344.330963  343.730804  344.312500   \n",
      "4 2025-05-13 20:35:00     CAT  344.300446  344.300446  344.062378  344.286224   \n",
      "\n",
      "   williams_r  rsi  ultimate_osc  ema  close_lag_1  close_lag_2  close_lag_3  \\\n",
      "0         NaN  NaN           NaN  NaN          NaN          NaN          NaN   \n",
      "1         NaN  NaN           NaN  NaN   343.201477          NaN          NaN   \n",
      "2         NaN  NaN           NaN  NaN   345.617310   343.201477          NaN   \n",
      "3         NaN  NaN           NaN  NaN   345.501709   345.617310   343.201477   \n",
      "4         NaN  NaN           NaN  NaN   344.312500   345.501709   345.617310   \n",
      "\n",
      "   close_lag_4  close_lag_5  \n",
      "0          NaN          NaN  \n",
      "1          NaN          NaN  \n",
      "2          NaN          NaN  \n",
      "3          NaN          NaN  \n",
      "4   343.201477          NaN  \n",
      "timestamp       2025-05-13 20:15:00\n",
      "company                         CAT\n",
      "open                     342.937531\n",
      "high                     343.201477\n",
      "low                      342.937531\n",
      "close                    343.201477\n",
      "williams_r                      NaN\n",
      "rsi                             NaN\n",
      "ultimate_osc                    NaN\n",
      "ema                             NaN\n",
      "close_lag_1                     NaN\n",
      "close_lag_2                     NaN\n",
      "close_lag_3                     NaN\n",
      "close_lag_4                     NaN\n",
      "close_lag_5                     NaN\n",
      "Name: 0, dtype: object\n",
      "Dropping NaN values...\n"
     ]
    }
   ],
   "source": [
    "pandas_df = a.toPandas()\n",
    "custom_resampler = calc_techincal_indicators(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(custom_resampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/13 21:16:46 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-c92e3eb1-1171-4496-bb94-87ff512079b5/pyspark-2db53aa6-0088-42bf-9c54-6ad15394ee3f. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-c92e3eb1-1171-4496-bb94-87ff512079b5/pyspark-2db53aa6-0088-42bf-9c54-6ad15394ee3f\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
