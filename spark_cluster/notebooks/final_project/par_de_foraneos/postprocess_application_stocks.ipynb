{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **ITESO** </center>\n",
    "# <center> **Final Project Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "## <center> **Spark Post Processing Applications** </center>\n",
    "## <center> **Real-Time Stock Price Analysis** </center>\n",
    "---\n",
    "## <center> **Par de Foraneos** </center>\n",
    "---\n",
    "#### <center> **Spring 2025** </center>\n",
    "---\n",
    "#### <center> **05/14/2025** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "**Students**: Eddie, Konrad , Diego, Aaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import os\n",
    "import glob\n",
    "from par_de_foraneos.stock_utils import calc_techincal_indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session and Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '2453c3db49e4',\n",
    "                'Aaron' : 'a5ab6bdab4b3'}\n",
    "KAFKA_SERVER = {'Konrad': '4c63f45c41b4:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"reader-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIND PARQUET FILES AND MAKE UNION WITHIN SAME COMPANY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_s = []\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    files = glob.glob(f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/*.parquet\")\n",
    "    #init df for union\n",
    "    df0 = spark.read.parquet(files[0])\n",
    "\n",
    "    for f_id in range(1,len(files)):\n",
    "        df1 = spark.read.parquet(files[f_id])\n",
    "        df0 = df0.union(df1)\n",
    "        \n",
    "    df_s.append(df0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE DUPLICATES CAUSED BY WATERMARKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    a  = df_s[i].orderBy(\"timestamp\", ascending=True).dropDuplicates([\"timestamp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCULATE INDICATORS AND SAVE AS PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating technical indicators...\n",
      "Creating lag features...\n",
      "Dropping NaN values...\n",
      "Calculating technical indicators...\n",
      "Creating lag features...\n",
      "Dropping NaN values...\n",
      "Calculating technical indicators...\n",
      "Creating lag features...\n",
      "Dropping NaN values...\n",
      "Calculating technical indicators...\n",
      "Creating lag features...\n",
      "Dropping NaN values...\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    pandas_df = df_s[i].toPandas()\n",
    "    custom_resampler = calc_techincal_indicators(pandas_df)\n",
    "    custom_resampler\n",
    "    custom_resampler.to_parquet(f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/final_indicator_df_{custom_resampler['company'].iloc[0]}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLOSE SESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
