{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **Streamer Structrure for final Proyect** </center>\n",
    "### <center> **Par de Foraneos** </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from foraneos.utils_proyectofinal_copy import resample_and_aggregate\n",
    "from foraneos.utils_proyectofinal_copy import SparkUtils as SpU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '672e28abb623',\n",
    "                'Aaron' : 'a5ab6bdab4b3'}\n",
    "KAFKA_SERVER = {'Konrad': 'dee5c9cc3710:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .load()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ), \n",
    "                                              (\"open\", \"float\" ), \n",
    "                                              (\"high\", \"float\" ), \n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" ) ,\n",
    "                                              (\"williams_r\", \"float\" ),  \n",
    "                                              (\"rsi\", \"float\"), \n",
    "                                              (\"ultimate_osc\", \"float\"), \n",
    "                                              (\"ema\", \"float\"), \n",
    "                                              (\"close_lag_1\", \"float\" ), \n",
    "                                              (\"close_lag_2\", \"float\" ), \n",
    "                                              (\"close_lag_3\", \"float\" ), \n",
    "                                              (\"close_lag_4\", \"float\" ), \n",
    "                                              (\"close_lag_5\", \"float\" ),                                                                                 \n",
    "                                              ])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, window, min, max, first, last\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "             \n",
    "    df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "    df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "    df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "           .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "           .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "          .select(\"timestamp\", \"company\",\"close\")\n",
    "          \n",
    "    df.printSchema()\n",
    "    aggregated_df = df \\\n",
    "        .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "        .groupBy( window(\"timestamp\", \"5 minutes\")  # 5-minute tumbling window\n",
    "        ) \\\n",
    "        .agg(\n",
    "            first(\"value\").alias(\"open\"),\n",
    "            max(\"value\").alias(\"high\"),\n",
    "            min(\"value\").alias(\"low\"),\n",
    "            last(\"value\").alias(\"close\")\n",
    "        )\n",
    "        \n",
    "    aggregated_df_indicators = aggregated_df \\\n",
    "        .withColumn(\"williams_r\", ta.momentum.WilliamsRIndicator(\n",
    "                high=aggregated_df['high'], low=aggregated_df['low'], close=aggregated_df['close']\n",
    "            ).williams_r()) \\\n",
    "        .withColumn(\"rsi\", ta.momentum.RSIIndicator(close=aggregated_df['close']).rsi()) \\\n",
    "        .withColumn(\"ultimate_osc\",  ta.momentum.UltimateOscillator(\n",
    "                high=aggregated_df['high'], low=aggregated_df['low'], close=aggregated_df['close']\n",
    "            ).ultimate_oscillator()) \\\n",
    "        .withColumn(\"ema\", ta.trend.EMAIndicator(close=aggregated_df['close'], window=14).ema_indicator()) \\\n",
    "        .withColumn(\"close_lag_1\", aggregated_df[\"close\"].shift(1)) \\\n",
    "        .withColumn(\"close_lag_2\", aggregated_df[\"close\"].shift(2)) \\\n",
    "        .withColumn(\"close_lag_3\", aggregated_df[\"close\"].shift(3)) \\\n",
    "        .withColumn(\"close_lag_4\", aggregated_df[\"close\"].shift(4)) \\\n",
    "        .withColumn(\"close_lag_5\", aggregated_df[\"close\"].shift(5))\n",
    "\n",
    "    streamer_df.append(aggregated_df_indicators)\n",
    "    aggregated_df_indicators.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermarking to handle late arrival events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "\n",
    "# for i in range(4):\n",
    "#     query.append(\n",
    "#         streamer_df[i] \\\n",
    "#         .writeStream \\\n",
    "#         .outputMode(\"append\") \\\n",
    "#         .trigger(processingTime='120 seconds') \\\n",
    "#         .format(\"parquet\") \\\n",
    "#         .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "#         .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "#         .start()\n",
    "#     )\n",
    "\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='30 seconds') \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "    )\n",
    "\n",
    "    #query[i].awaitTermination(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output0/part-00000-cc734dfc-75e1-436c-a412-6da58b98bef5-c000.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
