{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **ITESO** </center>\n",
    "# <center> **Final Project Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "## <center> **Streamer Applications** </center>\n",
    "## <center> **Real-Time Stock Price Analysis** </center>\n",
    "---\n",
    "## <center> **Par de Foraneos** </center>\n",
    "---\n",
    "#### <center> **Spring 2025** </center>\n",
    "---\n",
    "#### <center> **05/14/2025** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "**Students**: Eddie, Konrad , Diego, Aaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from foraneos.utils_proyectofinal_copy import resample_and_aggregate\n",
    "from foraneos.utils_proyectofinal_copy import SparkUtils as SpU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '2453c3db49e4',\n",
    "                'Aaron' : 'a5ab6bdab4b3'}\n",
    "KAFKA_SERVER = {'Konrad': '4c63f45c41b4:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .option(\"failOnDataLoss\", \"false\")\n",
    "                            .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Schema for Output DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ), \n",
    "                                     ('company', 'string'),\n",
    "                                              (\"open\", \"float\" ), \n",
    "                                              (\"high\", \"float\" ), \n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" )                                                                               \n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Sink Transformation on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "             \n",
    "        #split csv input and tranform into spark df     \n",
    "        df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))         \n",
    "        df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "        df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "                .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "                .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "                .select(\"timestamp\", \"company\",\"close\")\n",
    "          \n",
    "        #setup resampling window for UDF\n",
    "        custom_resampler = resample_and_aggregate(new_window=5)\n",
    "    \n",
    "        # creating a watermark window of 10mins - suficient for our 5min ressampling window\n",
    "        # and applying the custom resampling function\n",
    "        resampled_df = df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "                .groupBy(\"company\").applyInPandas(custom_resampler, schema=result_schema)\n",
    "\n",
    "        streamer_df.append(resampled_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "\n",
    "# write the stream to parquet files and store different writestreams in a list\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='120 seconds') \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "        .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP STREAMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
