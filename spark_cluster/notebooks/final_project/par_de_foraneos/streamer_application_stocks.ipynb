{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **ITESO** </center>\n",
    "# <center> **Final Project Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "## <center> **Streamer Applications** </center>\n",
    "## <center> **Real-Time Stock Price Analysis** </center>\n",
    "---\n",
    "## <center> **Par de Foraneos** </center>\n",
    "---\n",
    "#### <center> **Spring 2025** </center>\n",
    "---\n",
    "#### <center> **05/14/2025** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez <br>\n",
    "**Students**: Eddie, Konrad, Diego, Aaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Problem Definition\n",
    "\n",
    "# Introduction and Problem Definition\n",
    "\n",
    "The financial markets generate enormous volumes of data at high velocity, making them an ideal candidate for big data technologies. Our project addresses the challenge of processing, analyzing, and extracting actionable insights from real-time stock market data. We've developed a comprehensive system that captures stock price movements, processes them using distributed computing, and applies machine learning to predict trading signals.\n",
    "\n",
    "The primary objectives of this project are:\n",
    "\n",
    "1. **Data Ingestion**: Design a scalable system to ingest real-time stock price data from multiple sources using Kafka as the data streaming platform.\n",
    "\n",
    "2. **Data Processing**: Leverage Apache Spark's distributed computing capabilities to process and transform streaming data into usable formats.\n",
    "\n",
    "3. **Technical Analysis**: Calculate key technical indicators and financial metrics that provide insights into market trends and potential trading opportunities.\n",
    "\n",
    "4. **Machine Learning Implementation**: Apply machine learning models to predict future price movements and generate trading signals (BUY, SELL, WAIT).\n",
    "\n",
    "5. **Backtesting Framework**: Develop a backtesting system to evaluate and optimize trading strategies using historical data.\n",
    "\n",
    "6. **Performance Analysis**: Measure the effectiveness of our trading strategies using key financial metrics like Calmar ratio, Sharpe ratio, and Sortino ratio.\n",
    "\n",
    "Our application focuses on four major stocks (CAT, AAPL, NVDA, CVX) that represent different market sectors and exhibit varied patterns of volatility and price movement. This diversity allows us to test our system's robustness across different market conditions.\n",
    "\n",
    "The real-time nature of stock market data presents challenges in terms of volume, velocity, variety, veracity, and value - the 5Vs of Big Data. Our architecture is designed to address these challenges through a pipeline that connects data producers (simulating real-time stock prices), Kafka streaming, Spark processing, machine learning modeling, and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Architecture\n",
    "\n",
    "### Workflow\n",
    "1. Make sure Docker is set up and connected. Provide Spark and Kafka Server ID's in all necessary Notebooks.\n",
    "2. Run file called `producer_application_stocks.ipynb` to start the data streaming.\n",
    "3. While the stream is running, start notebook called `streamer_application_stocks.ipynb` to capture and persist the read data.\n",
    "4. Once the data lake is sufficient, stop the streaming and consumer notebooks.\n",
    "5. Finally, run file `postprocess_application_stocks.ipynb` to perform a union based operations on each ouput based on the stock's company.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "# <center> <img src=\"./images/BigData_Architecture.jpg\" alt=\"Project Architecture\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 V's Justification\n",
    "\n",
    "## Volume\n",
    "How the system handles large data volumes. Each\n",
    "team needs to compute the size of each produced record to do\n",
    "this analysis. \n",
    "\n",
    "Displayed send volumn is per topic, i.e. volume/stock.\n",
    "\n",
    "- Record Size: Each stock data record contains timestamp, symbol, price, volume, and market indicators (~101 bytes)\n",
    "- Ingestion Rate: Publishing 10x faster than real-world trading for stress testing (simulating high-frequency trading scenarios), so in a real application scale down volumn by 10 and scale up by total amount of stocks.\n",
    "- Scalability: The per-topic design allows linear scaling - adding more stocks simply requires additional topics/partitions\n",
    "\n",
    "| Time Period       | Data Processed       |\n",
    "|----------------|----------------|\n",
    "| 1 Second                      | 0.202 KB   |\n",
    "| 1 Minute (60 Seconds)         | 12.168 KB   |\n",
    "| 1 Hour (3,600 Seconds)         | 0.730 MB   |\n",
    "| 1 Day (86,400 Seconds)            | 17.112 MB  |\n",
    "| 1 Year (31.5 Million Seconds)  | 6.10 GB   |\n",
    "\n",
    "\n",
    "\n",
    "## Velocity\n",
    "The systemâ€™s ability to process streaming data in real-\n",
    "time. The performance can be obtained by using the processedRowsPerSecond info obtained from the event progress\n",
    "data (using QueryListeners)\n",
    "- Processing Rate: Sustains 28.3 rows/second per stock topic\n",
    "- Parallel Consumption: Spark structured streaming maintains 1:1 topic:task parallelism for optimal throughput\n",
    "\n",
    "## Variety\n",
    "The schema that were handling in the input of the stream consists on:\n",
    "- stock_text **(string)** - contains timestamp, stock-id, price\n",
    "\n",
    "The schema that were handling in the output of the stream consists on:\n",
    "- timestamp **(timestamp)**\n",
    "- company **(string)**\n",
    "- open **(float)**\n",
    "- high **(float)**\n",
    "- low **(float)**\n",
    "- close **(float)**\n",
    "\n",
    "The schema that were handling in the postprocess consists on:  \n",
    "- timestamp **(timestamp)**\n",
    "- company **(string)**\n",
    "- open **(float)**\n",
    "- high **(float)**\n",
    "- low **(float)**\n",
    "- close **(float)**\n",
    "- williams_r **(float)**\n",
    "- ultimate_osc **(float)** \n",
    "- rsi **(float)**\n",
    "- ema **(float)**\n",
    "- close_lag1 **(float)**\n",
    "- close_lag2 **(float)**\n",
    "- close_lag3 **(float)**\n",
    "- close_lag4 **(float)**\n",
    "- close_lag5 **(float)**\n",
    "\n",
    "## Value\n",
    "The output dataset from the stream contains OHLC stock prices on a 5min interval. They provide a snapshot of a stock's price movement over a specific time period. Thus, they allow an analysis of behaviour, volatility and key price levels. They can be used for technical statistics as well as trend and pattern analysis making them essential for traders and analysts. \n",
    "In our use case they are used to calculate technical indicators and price lags. Later, all of them are used to train a ML model to predict trading behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "## Technological Stack\n",
    "- `PySpark`: Python framework that allows us to work with large volumes of datasets.\n",
    "- `Kafka`: Distributed streaming platform for real-time data applications.\n",
    "- `Pandas`: Python library to create dataframes, data analysis and manipulation.\n",
    "- `Numpy`: Python library for scientific computing and data analysis. \n",
    "- `YFinance`: Yahoo Finance downloads real historical values from selected companies.\n",
    "- `Technical Analysis (ta)`: Python library to calculate specific financial metrics, such as RSI, EMA. \n",
    "\n",
    "## Design Choices\n",
    "- To get a realistic basis for our producer we download real stock prices from the internet. The last downloaded price serves as the initial price for the producer which then produces new prices using the Geometric Brownian Motion (GBM) model and semi-random risk-free interest rate and volatility. In this way we have a somewhat real producer scenario.\n",
    "- Our producer creates stock prices in a 5s interval. However, they are then published every 0.5s. This is due to convenience in the showcase of our application to produce a sufficient amount of data in a running interval. \n",
    "- the OHLC price statistics on a 5 minute interval are calculated directly within the kafka stream since they are just inplace-resampling operations. The technical indicators however, we had to outsource them to post-processing. This is due to a calculation window of those indicators which are usually 14 timesteps. Consequently, we would need at least 14 5 minute OHLC stock prices per batch to get one row of indicator values while loosing the first 13 OHLC prices. So even if we would process very large batches we would loose the first part of our streaming data in every batch which is not useful since it would create an incomplete datalake. \n",
    "- We outsourced the class containing the actual producer and the function to calculate the technical indicators in a file called `stock_utils.py`.\n",
    "- We also created a specific Jupyter Notebook that acts as the producer of data. \n",
    "\n",
    "## Optimizations\n",
    "- The Kafka streamer is serialized in 'utf-8' to avoid additional overhead that would be caused by casting such as json etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Evaluation\n",
    "\n",
    "Our analysis evaluates both the performance of the data processing pipeline and the effectiveness of the machine learning-based trading strategy.\n",
    "\n",
    "## Data Processing Performance\n",
    "\n",
    "The Spark structured streaming application demonstrated strong performance metrics when processing the real-time stock data:\n",
    "\n",
    "- **Processing Rate**: The system sustained an average of 28.3 rows per second per stock topic, with peaks reaching over 127 rows per second during high-velocity periods.\n",
    "- **Scalability**: The application successfully handled multiple parallel streams (one for each stock) without performance degradation.\n",
    "- **Latency**: The average processing time remained consistently low, with the streaming application efficiently transforming raw stock price data into OHLC (Open, High, Low, Close) format.\n",
    "\n",
    "## Machine Learning Model Performance\n",
    "\n",
    "Our SVM-based machine learning models showed impressive predictive capabilities across all four stocks:\n",
    "\n",
    "1. **CAT**: F1 Score of 0.928, with optimized technical indicators (RSI window: 9, Williams %R window: 10)\n",
    "2. **AAPL**: Perfect F1 Score of 1.0, indicating exceptional predictive accuracy\n",
    "3. **NVDA**: Perfect F1 Score of 1.0, with strong performance across all evaluation metrics\n",
    "4. **CVX**: F1 Score of 0.777, still demonstrating good predictive power\n",
    "\n",
    "The high F1 scores indicate that our models effectively learned the patterns in the stock price movements and could accurately predict the optimal trading signals.\n",
    "\n",
    "## Trading Strategy Performance\n",
    "\n",
    "The backtesting results demonstrate the effectiveness of our trading strategy:\n",
    "\n",
    "1. **Individual Stock Performance**:\n",
    "   - **NVDA**: Highest return at 10.27%, with an exceptional Calmar ratio of 11,580.89\n",
    "   - **CVX**: 5.73% return with a Calmar ratio of 179.02\n",
    "   - **AAPL**: 5.68% return with a Calmar ratio of 2,697.71\n",
    "   - **CAT**: 3.83% return with a Calmar ratio of 1,346.93\n",
    "\n",
    "2. **Portfolio Performance**:\n",
    "   - **Overall Return**: 6.38% for the equally-weighted portfolio\n",
    "   - **Risk-Adjusted Metrics**: Average Calmar ratio of 3,951.14, average Sharpe ratio of 8.14, and average Sortino ratio of 160.39\n",
    "\n",
    "These metrics indicate that our strategy not only generates positive returns but does so with minimal drawdowns and controlled risk. The extremely high Calmar ratios suggest that the strategy effectively manages risk while capturing profit opportunities.\n",
    "\n",
    "3. **Optimization Effectiveness**:\n",
    "   - **Technical Indicators**: Optuna-based optimization identified unique parameters for each stock, significantly enhancing model performance.\n",
    "   - **Trading Parameters**: Stop-loss and take-profit parameters were optimized to values generally between 1-2%, indicating that quick profit-taking and tight loss control were crucial for success.\n",
    "\n",
    "Our portfolio visualization and analysis demonstrate that the machine learning approach outperforms traditional technical analysis-based trading strategies, with higher returns and better risk management metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we've applied machine learning techniques to our real-time stock price analysis system. We've demonstrated how to:\n",
    "\n",
    "1. Load and prepare data from either our processed parquet files or historical sources\n",
    "2. Generate trading signals based on price movements\n",
    "3. Optimize technical indicators and model parameters\n",
    "4. Train SVM models to predict trading signals\n",
    "5. Backtest trading strategies with optimized parameters\n",
    "6. Analyze and visualize portfolio performance\n",
    "\n",
    "The results show that our approach can potentially generate profitable trading strategies, though real-world implementation would require additional considerations such as transaction costs, market impact, and risk management.\n",
    "\n",
    "This completes our Big Data-based stock trading system, which integrates data streaming, processing, machine learning, and backtesting components.\n",
    "\n",
    "### Challenges\n",
    "- The biggest challenge was the calculation of the techincal indicators. At first we tried to include them directly into the streaming process. This however produced empty output. Since spark doesn't allow printing commands it was hard to figure out that due to the spinup window of 14 timesteps in the indicators, the batches stayed empty with NaN values. After this debugging process and the conclusion that we need a complete datalake we decided to outsource the indicators to the postprocessing in spark applying the calculations to the whole datalake. \n",
    "- Furthermore, during the coding and debugging process and the running producer, the streaming cache of kafka grew heavily in memory which in one moment caused the docker application to crash due to missing memory on the disk. Consequently, we had to reinstall the docker application and make some settings to avoid congestion in the future due to streaming data.\n",
    "- We applied all the learned concepts throughout the semester into one final application, and we think that that's the most important part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from par_de_foraneos.stock_utils import resample_and_aggregate\n",
    "from par_de_foraneos.stock_utils import SparkUtils as SpU\n",
    "from par_de_foraneos.stock_utils import ProgressListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '2453c3db49e4',\n",
    "                'Aaron' : 'a5ab6bdab4b3',\n",
    "                'Diego': '368ad5a83fd7'}\n",
    "KAFKA_SERVER = {'Konrad': '4c63f45c41b4:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093',\n",
    "                'Diego' : 'a27c998f34f5:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .option(\"failOnDataLoss\", \"false\")\n",
    "                            .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Schema for Output DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ),\n",
    "                                     ('company', 'string'),\n",
    "                                              (\"open\", \"float\" ),\n",
    "                                              (\"high\", \"float\" ),\n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" )\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Output Transformations on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "        #split csv input and tranform into spark df\n",
    "        df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "        df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "        df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "                .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "                .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "                .select(\"timestamp\", \"company\",\"close\")\n",
    "\n",
    "        #setup resampling window for UDF\n",
    "        custom_resampler = resample_and_aggregate(new_window=5)\n",
    "\n",
    "        # creating a watermark window of 10mins - suficient for our 5min ressampling window\n",
    "        # and applying the custom resampling function\n",
    "        resampled_df = df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "                .groupBy(\"company\").applyInPandas(custom_resampler, schema=result_schema)\n",
    "\n",
    "        streamer_df.append(resampled_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY LISTENER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get processed lines per second\n",
    "spark.streams.addListener(ProgressListener())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = []\n",
    "\n",
    "# write the stream to parquet files and store different writestreams in a list\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='120 seconds') \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "        .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP STREAMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
