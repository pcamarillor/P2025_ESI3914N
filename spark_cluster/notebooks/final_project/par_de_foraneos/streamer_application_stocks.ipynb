{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **ITESO** </center>\n",
    "# <center> **Final Project Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "## <center> **Streamer Applications** </center>\n",
    "## <center> **Real-Time Stock Price Analysis** </center>\n",
    "---\n",
    "## <center> **Par de Foraneos** </center>\n",
    "---\n",
    "#### <center> **Spring 2025** </center>\n",
    "---\n",
    "#### <center> **05/14/2025** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez <br>\n",
    "**Students**: Eddie, Konrad, Diego, Aaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Problem Definition\n",
    "\n",
    "### **PENDING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Architecture\n",
    "\n",
    "### Workflow\n",
    "1. Make sure Docker is set up and connected. Provide Spark and Kafka Server ID's in all necessary Notebooks.\n",
    "2. Run file called `producer_application_stocks.ipynb` to start the data streaming.\n",
    "3. While the stream is running, start notebook called `streamer_application_stocks.ipynb` to capture and persist the read data.\n",
    "4. Once the data lake is sufficient, stop the streaming and consumer notebooks.\n",
    "5. Finally, run file `postprocess_application_stocks.ipynb` to perform a union based operations on each ouput based on the stock's company.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "# <center> <img src=\"./images/BigData_Architecture.jpg\" alt=\"Project Architecture\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 V's Justification\n",
    "\n",
    "## Volume\n",
    "How the system handles large data volumes. Each\n",
    "team needs to compute the size of each produced record to do\n",
    "this analysis. \n",
    "\n",
    "Displayed send volumn is per topic, i.e. volume/stock.\n",
    "\n",
    "- Record Size: Each stock data record contains timestamp, symbol, price, volume, and market indicators (~101 bytes)\n",
    "- Ingestion Rate: Publishing 10x faster than real-world trading for stress testing (simulating high-frequency trading scenarios), so in a real application scale down volumn by 10 and scale up by total amount of stocks.\n",
    "- Scalability: The per-topic design allows linear scaling - adding more stocks simply requires additional topics/partitions\n",
    "\n",
    "| Time Period       | Data Processed       |\n",
    "|----------------|----------------|\n",
    "| 1 Second                      | 0.202 KB   |\n",
    "| 1 Minute (60 Seconds)         | 12.168 KB   |\n",
    "| 1 Hour (3,600 Seconds)         | 0.730 MB   |\n",
    "| 1 Day (86,400 Seconds)            | 17.112 MB  |\n",
    "| 1 Year (31.5 Million Seconds)  | 6.10 GB   |\n",
    "\n",
    "\n",
    "\n",
    "## Velocity\n",
    "The systemâ€™s ability to process streaming data in real-\n",
    "time. The performance can be obtained by using the processedRowsPerSecond info obtained from the event progress\n",
    "data (using QueryListeners)\n",
    "- Processing Rate: Sustains 28.3 rows/second per stock topic\n",
    "- Parallel Consumption: Spark structured streaming maintains 1:1 topic:task parallelism for optimal throughput\n",
    "\n",
    "## Variety\n",
    "The schema that were handling in the input of the stream consists on:\n",
    "- stock_text **(string)** - contains timestamp, stock-id, price\n",
    "\n",
    "The schema that were handling in the output of the stream consists on:\n",
    "- timestamp **(timestamp)**\n",
    "- company **(string)**\n",
    "- open **(float)**\n",
    "- high **(float)**\n",
    "- low **(float)**\n",
    "- close **(float)**\n",
    "\n",
    "The schema that were handling in the postprocess consists on:  \n",
    "- timestamp **(timestamp)**\n",
    "- company **(string)**\n",
    "- open **(float)**\n",
    "- high **(float)**\n",
    "- low **(float)**\n",
    "- close **(float)**\n",
    "- williams_r **(float)**\n",
    "- ultimate_osc **(float)** \n",
    "- rsi **(float)**\n",
    "- ema **(float)**\n",
    "- close_lag1 **(float)**\n",
    "- close_lag2 **(float)**\n",
    "- close_lag3 **(float)**\n",
    "- close_lag4 **(float)**\n",
    "- close_lag5 **(float)**\n",
    "\n",
    "## Value\n",
    "The output dataset from the stream contains OHLC stock prices on a 5min interval. They provide a snapshot of a stock's price movement over a specific time period. Thus, they allow an analysis of behaviour, volatility and key price levels. They can be used for technical statistics as well as trend and pattern analysis making them essential for traders and analysts. \n",
    "In our use case they are used to calculate technical indicators and price lags. Later, all of them are used to train a ML model to predict trading behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "## Technological Stack\n",
    "- `PySpark`: Python framework that allows us to work with large volumes of datasets.\n",
    "- `Kafka`: Distributed streaming platform for real-time data applications.\n",
    "- `Pandas`: Python library to create dataframes, data analysis and manipulation.\n",
    "- `Numpy`: Python library for scientific computing and data analysis. \n",
    "- `YFinance`: Yahoo Finance downloads real historical values from selected companies.\n",
    "- `Technical Analysis (ta)`: Python library to calculate specific financial metrics, such as RSI, EMA. \n",
    "\n",
    "## Design Choices\n",
    "- To get a realistic basis for our producer we download real stock prices from the internet. The last downloaded price serves as the initial price for the producer which then produces new prices using the Geometric Brownian Motion (GBM) model and semi-random risk-free interest rate and volatility. In this way we have a somewhat real producer scenario.\n",
    "- Our producer creates stock prices in a 5s interval. However, they are then published every 0.5s. This is due to convenience in the showcase of our application to produce a sufficient amount of data in a running interval. \n",
    "- the OHLC price statistics on a 5 minute interval are calculated directly within the kafka stream since they are just inplace-resampling operations. The technical indicators however, we had to outsource them to post-processing. This is due to a calculation window of those indicators which are usually 14 timesteps. Consequently, we would need at least 14 5 minute OHLC stock prices per batch to get one row of indicator values while loosing the first 13 OHLC prices. So even if we would process very large batches we would loose the first part of our streaming data in every batch which is not useful since it would create an incomplete datalake. \n",
    "- We outsourced the class containing the actual producer and the function to calculate the technical indicators in a file called `stock_utils.py`.\n",
    "- We also created a specific Jupyter Notebook that acts as the producer of data. \n",
    "\n",
    "## Optimizations\n",
    "- The Kafka streamer is serialized in 'utf-8' to avoid additional overhead that would be caused by casting such as json etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Evaluation\n",
    "\n",
    "### **PENDING**  \n",
    "Analysis of the ML model performance\n",
    "(precision, accuracy, recall, etc.) and functionality of the application.\n",
    "In this section you need to paste your PowerBI Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "### Challenges\n",
    "- The biggest challenge was the calculation of the techincal indicators. At first we tried to include them directly into the streaming process. This however produced empty output. Since spark doesn't allow printing commands it was hard to figure out that due to the spinup window of 14 timesteps in the indicators, the batches stayed empty with NaN values. After this debugging process and the conclusion that we need a complete datalake we decided to outsource the indicators to the postprocessing in spark applying the calculations to the whole datalake. \n",
    "- Furthermore, during the coding and debugging process and the running producer, the streaming cache of kafka grew heavily in memory which in one moment caused the docker application to crash due to missing memory on the disk. Consequently, we had to reinstall the docker application and make some settings to avoid congestion in the future due to streaming data.\n",
    "- We applied all the learned concepts throughout the semester into one final application, and we think that that's the most important part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from par_de_foraneos.stock_utils import resample_and_aggregate\n",
    "from par_de_foraneos.stock_utils import SparkUtils as SpU\n",
    "from par_de_foraneos.stock_utils import ProgressListener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6fcfff10-735c-4b53-80d4-23e7a7508fce;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.5 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 907ms :: artifacts dl 27ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.5 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6fcfff10-735c-4b53-80d4-23e7a7508fce\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/18ms)\n",
      "25/05/14 16:44:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '2453c3db49e4',\n",
    "                'Aaron' : 'a5ab6bdab4b3',\n",
    "                'Diego': '368ad5a83fd7'}\n",
    "KAFKA_SERVER = {'Konrad': '4c63f45c41b4:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093',\n",
    "                'Diego' : 'a27c998f34f5:9093'}\n",
    "current_user = 'Konrad'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .option(\"failOnDataLoss\", \"false\")\n",
    "                            .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Schema for Output DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ),\n",
    "                                     ('company', 'string'),\n",
    "                                              (\"open\", \"float\" ),\n",
    "                                              (\"high\", \"float\" ),\n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" )\n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Output Transformations on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "        #split csv input and tranform into spark df\n",
    "        df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))\n",
    "        df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "        df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "                .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "                .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "                .select(\"timestamp\", \"company\",\"close\")\n",
    "\n",
    "        #setup resampling window for UDF\n",
    "        custom_resampler = resample_and_aggregate(new_window=5)\n",
    "\n",
    "        # creating a watermark window of 10mins - suficient for our 5min ressampling window\n",
    "        # and applying the custom resampling function\n",
    "        resampled_df = df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "                .groupBy(\"company\").applyInPandas(custom_resampler, schema=result_schema)\n",
    "\n",
    "        streamer_df.append(resampled_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUERY LISTENER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.streams.addListener(ProgressListener())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 16:44:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started: 7e8e3b5d-3d00-489e-bc81-a3a079681298\n",
      "Query started: 2bf848d3-15f0-45da-966d-ac6b840170d5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 16:44:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/14 16:44:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started: cad52912-181d-4478-a5d0-8b398d29199c\n",
      "Query started: db275114-d087-4020-85f1-95acb87d5bf4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 16:44:44 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 16:44:46 WARN OffsetSeqMetadata: Updating the value of conf 'spark.sql.shuffle.partitions' in current session from '5' to '200'.\n",
      "25/05/14 16:44:46 WARN OffsetSeqMetadata: Updating the value of conf 'spark.sql.shuffle.partitions' in current session from '5' to '200'.\n",
      "25/05/14 16:44:46 WARN OffsetSeqMetadata: Updating the value of conf 'spark.sql.shuffle.partitions' in current session from '5' to '200'.\n",
      "25/05/14 16:44:46 WARN OffsetSeqMetadata: Updating the value of conf 'spark.sql.shuffle.partitions' in current session from '5' to '200'.\n",
      "25/05/14 16:44:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/14 16:44:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/14 16:44:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/05/14 16:44:47 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 3:>  (0 + 0) / 200][Stage 5:>  (0 + 0) / 200][Stage 7:> (50 + 2) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 127.8904217192793 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                (0 + 0) / 200][Stage 5:============>  (164 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 104.27165826733862 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====================================================> (193 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 95.43094878686351 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 88.81478204648313 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>  (0 + 0) / 200][Stage 11:(110 + 2) / 200][Stage 15:> (0 + 0) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 26.8857356235997 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:========>      (112 + 2) / 200][Stage 15:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 20.411055988660525 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:===================================>                  (132 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 15.94507806444469 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 13.298854820834872 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>(87 + 2) / 200][Stage 21:> (1 + 0) / 200][Stage 23:> (0 + 0) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 34.2563516985441 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:=======>       (94 + 3) / 200][Stage 23:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 27.238678924072182 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:========================>                              (88 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 22.628700735432773 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 19.510608893585886 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>(88 + 2) / 200][Stage 29:> (0 + 0) / 200][Stage 31:> (0 + 0) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 39.66804979253112 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:===>           (50 + 2) / 200][Stage 31:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 30.59395801331285 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:====================================>                 (137 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 22.41605702494841 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 19.275748044197115 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:(133 + 2) / 200][Stage 37:> (0 + 0) / 200][Stage 39:> (0 + 0) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 42.09787756533942 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=====>         (70 + 2) / 200][Stage 39:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 32.68864069735767 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================>                (138 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 23.469587326422843 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 20.09545340366742 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:(153 + 2) / 200][Stage 44:>   (0 + 0) / 1][Stage 47:> (0 + 0) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 36.57790021426385 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:===>           (41 + 2) / 200][Stage 47:>               (4 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 30.21109847048414 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:============================================>         (164 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 23.95448647569618 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 20.764552562988705 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>(70 + 2) / 200][Stage 53:> (0 + 0) / 200][Stage 54:>   (0 + 0) / 1]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 39.900249376558605 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 53:=========>    (129 + 2) / 200][Stage 54:>                 (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 26.081286676809388 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:===============================>                      (115 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 21.73716148899556 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 18.37330873308733 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:> (0 + 0) / 200][Stage 61:>(89 + 2) / 200][Stage 63:> (0 + 0) / 200]  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 43.51784413692644 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:===>           (51 + 2) / 200][Stage 63:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 33.47280334728033 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:=====================================>                (138 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 23.981537226570342 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 20.282261472154143 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:>(10 + 1) / 200][Stage 69:>(37 + 1) / 200][Stage 71:> (0 + 0) / 200]0]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 42.16654904728299 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 67:===========>  (165 + 2) / 200][Stage 71:>               (0 + 0) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 26.99040090344438 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 71:====================================>                 (135 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 23.476474616061818 rows processed per second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: 20.302850858641403 rows processed per second\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "\n",
    "# write the stream to parquet files and store different writestreams in a list\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='120 seconds') \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "        .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP STREAMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query terminated: 7e8e3b5d-3d00-489e-bc81-a3a079681298\n",
      "Query terminated: 2bf848d3-15f0-45da-966d-ac6b840170d5\n",
      "Query terminated: cad52912-181d-4478-a5d0-8b398d29199c\n",
      "Query terminated: db275114-d087-4020-85f1-95acb87d5bf4\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
