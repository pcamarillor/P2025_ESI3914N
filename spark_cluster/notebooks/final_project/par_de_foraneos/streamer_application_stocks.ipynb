{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> **ITESO** </center>\n",
    "# <center> **Final Project Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "## <center> **Streamer Applications** </center>\n",
    "## <center> **Real-Time Stock Price Analysis** </center>\n",
    "---\n",
    "## <center> **Par de Foraneos** </center>\n",
    "---\n",
    "#### <center> **Spring 2025** </center>\n",
    "---\n",
    "#### <center> **05/14/2025** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "**Students**: Eddie, Konrad , Diego, Aaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Problem Definition\n",
    "\n",
    "### **PENDING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Architecture\n",
    "\n",
    "### Workflow\n",
    "1. Make sure Docker is set up and connected.\n",
    "2. Run file called `producer_application_stocks.ipynb` to start the data streaming.\n",
    "3. While the stream is running, start notebook called `streamer_application_stocks.ipynb` to capture and persist the read data.\n",
    "4. Once the data lake is sufficient, stop the streaming and consumer notebooks.\n",
    "5. Finally, run file `postprocess_application_stocks.ipynb` to perform a union based operations on each ouput based on the stock's company.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "# <center> <img src=\"./images/BigData_Architecture.jpg\" alt=\"Project Architecture\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 V's Justification\n",
    "\n",
    "## Volume\n",
    "### **PENDING**\n",
    "How the system handles large data volumes. Each\n",
    "team needs to compute the size of each produced record to to\n",
    "this analysis. \n",
    "\n",
    "## Velocity\n",
    "### **PENDING**\n",
    "The systemâ€™s ability to process streaming data in real-\n",
    "time. The performance can be obtained by using the processedRowsPerSecond info obtained from the event progress\n",
    "data (using QueryListeners)\n",
    "\n",
    "## Variety\n",
    "The schema that were handling consists on:  \n",
    "- timestamp\n",
    "- company \n",
    "- open \n",
    "- high \n",
    "- low \n",
    "- close\n",
    "- williams_r \n",
    "- ultimate_osc \n",
    "- rsi \n",
    "- ema \n",
    "- close_lag1 \n",
    "- close_lag2 \n",
    "- close_lag3 \n",
    "- close_lag4 \n",
    "- close_lag5 \n",
    "\n",
    "## Veracity\n",
    "As said before, this data is historical, extraced directly from Yahoo Finance. This ensures that the data that were getting for our application is real. \n",
    "\n",
    "## Value\n",
    "The data stored calculates the metrics listed above, using the Technical Analysis library. This allows for more specific insights for us to analyze and deliver. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "## Technological Stack\n",
    "- `PySpark`: Python framework that allows us to work with large volumes of datasets.\n",
    "- `Kafka`: Distributed streaming platform for real-time data applications.\n",
    "- `Pandas`: Python library to create dataframes, data analysis and manipulation.\n",
    "- `Numpy`: Python library for scientific computing and data analysis. \n",
    "- `YFinance`: Yahoo Finance downloads real historical values from selected companies.\n",
    "- `Technical Analysis (ta)`: Python library to calculate specific financial metrics, such as RSI, EMA. \n",
    "\n",
    "## Design Choices\n",
    "- We created a file called `stock_utils.py` that calculates the mentioned metrics with the data obtained. \n",
    "- We also created a specific Jupyter Notebook that acts as the producer of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Evaluation\n",
    "\n",
    "### **PENDING**  \n",
    "Analysis of the ML model performance\n",
    "(precision, accuracy, recall, etc.) and functionality of the application.\n",
    "In this section you need to paste your PowerBI Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "This project was a challenge, specially to read all the collected data correctly. The fact that we used real, historic data, adds value to the project but was also what kept us stuck for some time.  \n",
    "We applied all the learned concepts throughout the semester into one final application, and we think that that's the most important part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import ta\n",
    "from foraneos.stock_utils import resample_and_aggregate\n",
    "from foraneos.stock_utils import SparkUtils as SpU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "SPARK_SERVER = {'Konrad': '2453c3db49e4',\n",
    "                'Aaron' : 'a5ab6bdab4b3',\n",
    "                'Diego': '368ad5a83fd7'}\n",
    "KAFKA_SERVER = {'Konrad': '4c63f45c41b4:9093',\n",
    "                'Aaron' : '69b1b3611d90:9093',\n",
    "                'Diego' : 'a27c998f34f5:9093'}\n",
    "current_user = 'Diego'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(SPARK_SERVER[current_user])) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "sc = spark.sparkContext\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka Stream creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_lines = []\n",
    "\n",
    "for i in range(4):\n",
    "    streamer_lines.append( spark \\\n",
    "                            .readStream \\\n",
    "                            .format(\"kafka\") \\\n",
    "                            .option(\"kafka.bootstrap.servers\", \"{}\".format(KAFKA_SERVER[current_user])) \\\n",
    "                            .option(\"subscribe\", f\"stock_topic{i}\") \\\n",
    "                            .option(\"failOnDataLoss\", \"false\")\n",
    "                            .load()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Schema for Output DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result_schema = SpU.generate_schema([(\"timestamp\", \"timestamp\" ), \n",
    "                                     ('company', 'string'),\n",
    "                                              (\"open\", \"float\" ), \n",
    "                                              (\"high\", \"float\" ), \n",
    "                                              (\"low\", \"float\"),\n",
    "                                              (\"close\", \"float\" )                                                                               \n",
    "                                              ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Sink Transformation on DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "streamer_df = []\n",
    "\n",
    "for i in range(4):\n",
    "             \n",
    "        #split csv input and tranform into spark df     \n",
    "        df = streamer_lines[i].withColumn(\"value_str\", col(\"value\").cast(\"string\"))         \n",
    "        df = df.withColumn(\"split\", split(col(\"value_str\"), \",\"))\n",
    "        df = df.withColumn(\"timestamp\", col(\"split\").getItem(0).cast(TimestampType())) \\\n",
    "                .withColumn(\"company\", col(\"split\").getItem(1)) \\\n",
    "                .withColumn(\"close\", col(\"split\").getItem(2).cast(DoubleType())) \\\n",
    "                .select(\"timestamp\", \"company\",\"close\")\n",
    "          \n",
    "        #setup resampling window for UDF\n",
    "        custom_resampler = resample_and_aggregate(new_window=5)\n",
    "    \n",
    "        # creating a watermark window of 10mins - suficient for our 5min ressampling window\n",
    "        # and applying the custom resampling function\n",
    "        resampled_df = df.withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "                .groupBy(\"company\").applyInPandas(custom_resampler, schema=result_schema)\n",
    "\n",
    "        streamer_df.append(resampled_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/14 00:43:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/14 00:43:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/14 00:43:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/05/14 00:43:37 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = []\n",
    "\n",
    "# write the stream to parquet files and store different writestreams in a list\n",
    "for i in range(4):\n",
    "    query.append(\n",
    "        streamer_df[i] \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime='120 seconds') \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"path\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/output{i}/\")\n",
    "        .option(\"checkpointLocation\", f\"/home/jovyan/notebooks/data/final_project_ParDeForaneos/checkpoints/stock_topic{i}\") \\\n",
    "        .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP STREAMERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    query[i].stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
