{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **Examples on manipulating columns** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQL-Columns\") \\\n",
    "    .master(\"spark://078b2e28e517:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load e-commerce dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "\n",
    "columns_info = [(\"product\", \"string\"),\n",
    "                (\"price\", \"double\"),\n",
    "                (\"quantity\", \"integer\"),\n",
    "                (\"discount\", \"float\"),\n",
    "                (\"customer_first_name\", \"string\"),\n",
    "                (\"order_date\", \"date\")]\n",
    "\n",
    "schema = SparkUtils.generate_schema(columns_info)\n",
    "\n",
    "# Create DataFrame\n",
    "ecommerce_df = spark \\\n",
    "                .read \\\n",
    "                .schema(schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/e_commerce_dataset/\")\n",
    "\n",
    "ecommerce_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecommerce_df.show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column with a constant value\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_with_lit = ecommerce_df.withColumn('new_column', lit(10))\n",
    "df_with_lit.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unnecessary Columns\n",
    "df_without_lit = ecommerce_df.drop('new_column')\n",
    "df_without_lit.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new columns to add new data based on existing columns.\n",
    "ecommerce_df = ecommerce_df.withColumn('total_cost', ecommerce_df['price'] * ecommerce_df['quantity'])\n",
    "ecommerce_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply conditions using 'when' to populate a new column.\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "ecommerce_df = ecommerce_df.withColumn('is_high_order', when(ecommerce_df['total_cost'] > 2500, \"YES\").otherwise(\"NO\"))\n",
    "ecommerce_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'concat' to combine strings from multiple columns.\n",
    "from pyspark.sql.functions import concat\n",
    "\n",
    "df = ecommerce_df.select(\"customer_first_name\", \"product\").withColumn(\"label\", concat(ecommerce_df[\"customer_first_name\"], lit(\" bought:\"), ecommerce_df[\"product\"]))\n",
    "df.select(\"label\").show(n=5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract parts of a date using functions like year, month, day of month.\n",
    "from pyspark.sql.functions import year, month\n",
    "df = ecommerce_df.select(\"product\", \"order_date\").withColumn('order_year', year(ecommerce_df['order_date']))\n",
    "df = ecommerce_df.withColumn('order_month', month(ecommerce_df['order_date']))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the Data Type of a Column\n",
    "df = ecommerce_df.withColumn('price_int', ecommerce_df.price.cast('integer'))\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns\n",
    "df = df.withColumnRenamed('price_int', 'price_integer')\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating JSON columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data with JSON strings\n",
    "data = [\n",
    "    ('1', '{\"name\": \"Alice\", \"age\": 25, \"payments\": [34, 433, 54], \"address\": {\"city\": \"New York\", \"zip\": \"10001\"}}'),\n",
    "    ('2', '{\"name\": \"Bob\", \"age\": 30, \"address\": {\"city\": \"Los Angeles\", \"zip\": \"90001\"}}'),\n",
    "    ('3', '{\"name\": \"Charlie\", \"age\": 35, \"address\": {\"city\": \"Chicago\", \"zip\": \"60601\"}}')\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"id\", \"json_col\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.printSchema()\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use json_extract to extract JSON fields as JSON strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "df.withColumn(\"name\", get_json_object(df.json_col, '$.name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create city column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1st payment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkContext\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
