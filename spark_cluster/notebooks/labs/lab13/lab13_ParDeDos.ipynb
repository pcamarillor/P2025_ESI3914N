{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a947bca",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Lab 13** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez </br>\n",
    "**Team**: Par de Dos </br>\n",
    "**Members**: Diego Orozco and Aarón Ortega\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097a815",
   "metadata": {},
   "source": [
    "### FindSpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "951a6572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d214a",
   "metadata": {},
   "source": [
    "### Connection with SparkCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77dc9f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/05 03:05:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab13_ParDeDos\") \\\n",
    "    .master(\"spark://368ad5a83fd7:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc1082c",
   "metadata": {},
   "source": [
    "### Schema and DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b29dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_ParDeDos.spark_utils import SparkUtils as SpU\n",
    "\n",
    "schema = SpU.generate_schema([\n",
    "  (\"Alcohol\", \"double\"),\n",
    "  (\"Malic_Acid\", \"double\"),\n",
    "  (\"Ash\", \"double\"),\n",
    "  (\"Ash_Alcanity\", \"float\"),\n",
    "  (\"Magnesium\", \"integer\"),\n",
    "  (\"Total_Phenols\", \"double\"),\n",
    "  (\"Flavanoids\", \"double\"),\n",
    "  (\"Nonflavanoid_Phenols\", \"double\"),\n",
    "  (\"Proanthocyanins\", \"double\"),\n",
    "  (\"Color_Intensity\", \"double\"),\n",
    "  (\"Hue\", \"double\"),\n",
    "  (\"0D280\", \"double\"),\n",
    "  (\"Proline\", \"integer\")\n",
    "])\n",
    "\n",
    "wine_df = spark \\\n",
    "          .read \\\n",
    "          .schema(schema) \\\n",
    "          .option(\"header\", \"true\") \\\n",
    "          .option(\"mode\", \"dropMalformed\") \\\n",
    "          .csv(\"/home/jovyan/notebooks/data/wine-clustering.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b178f2",
   "metadata": {},
   "source": [
    "### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "247d9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"Alcohol\", \"Malic_Acid\", \"Ash\", \"Ash_Alcanity\", \"Magnesium\", \"Total_Phenols\", \"Flavanoids\", \"Nonflavanoid_Phenols\", \"Proanthocyanins\", \"Color_Intensity\", \"Hue\", \"0D280\", \"Proline\"], outputCol=\"features\")\n",
    "\n",
    "assembled_df = assembler.transform(wine_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c173e5",
   "metadata": {},
   "source": [
    "### Initialize KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9108e9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans().setK(3).setSeed(19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3274ec",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3812ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/05 03:06:54 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/05/05 03:07:01 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/05/05 03:07:01 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "model = kmeans.fit(assembled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5232e2",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979bfcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(assembled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296bf8c7",
   "metadata": {},
   "source": [
    "### Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c41fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.7274114110259972\n",
      "Cluster Centers: \n",
      "[1.25985294e+01 2.45343137e+00 2.32186275e+00 2.06460784e+01\n",
      " 9.36960784e+01 2.05362745e+00 1.64754902e+00 3.95980392e-01\n",
      " 1.42509804e+00 4.67333332e+00 9.17843137e-01 2.39480392e+00\n",
      " 5.21558824e+02]\n",
      "[1.33691837e+01 2.40000000e+00 2.39265306e+00 1.85142857e+01\n",
      " 1.09081633e+02 2.44163265e+00 2.21367347e+00 3.25510204e-01\n",
      " 1.70673469e+00 5.18836735e+00 9.59714286e-01 2.84795918e+00\n",
      " 9.06346939e+02]\n",
      "[1.38507407e+01 1.77851852e+00 2.48777778e+00 1.69259260e+01\n",
      " 1.05629630e+02 2.94148148e+00 3.13666667e+00 2.98888889e-01\n",
      " 2.00703704e+00 6.27518519e+00 1.10296296e+00 3.00222222e+00\n",
      " 1.30877778e+03]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(f\"Silhouette Score: {silhouette}\")\n",
    "\n",
    "# Show results\n",
    "print(\"Cluster Centers: \")\n",
    "for center in model.clusterCenters():\n",
    "  print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c43fa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
