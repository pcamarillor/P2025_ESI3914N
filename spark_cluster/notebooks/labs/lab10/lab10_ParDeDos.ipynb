{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **Lab 10** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez </br>\n",
    "**Team**: Par de Dos  \n",
    "**Members**: Diego Orozco and Aarón Ortega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexión con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/29 01:38:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lab10_ParDeDos\") \\\n",
    "    .master(\"spark://368ad5a83fd7:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team_ParDeDos.spark_utils import SparkUtils as SpU\n",
    "# Columns: male,age,education,currentSmoker,cigsPerDay,BPMeds,prevalentStroke,prevalentHyp,diabetes,totChol,sysBP,diaBP,BMI,heartRate,glucose,TenYearCHD\n",
    "\n",
    "# Define schema for the DataFrame\n",
    "schema = SpU.generate_schema([\n",
    "        (\"male\", \"integer\"),\n",
    "        (\"age\", \"integer\"),\n",
    "        (\"education\", \"integer\"),\n",
    "        (\"currentSmoker\", \"integer\"),\n",
    "        (\"cigsPerDay\", \"integer\"),\n",
    "        (\"BPMeds\", \"integer\"),\n",
    "        (\"prevalentStroke\", \"integer\"),\n",
    "        (\"prevalentHyp\", \"integer\"),\n",
    "        (\"diabetes\", \"integer\"),\n",
    "        (\"totChol\", \"integer\"),\n",
    "        (\"sysBP\", \"integer\"),\n",
    "        (\"diaBP\", \"integer\"),\n",
    "        (\"BMI\", \"integer\"),\n",
    "        (\"heartRate\", \"integer\"),\n",
    "        (\"glucose\", \"integer\"),\n",
    "        (\"TenYearCHD\", \"double\")\n",
    "    ])\n",
    "\n",
    "# Create DataFrame\n",
    "framingham = spark \\\n",
    "                .read \\\n",
    "                .schema(schema) \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .csv(\"/home/jovyan/notebooks/data/framingham.csv\")\n",
    "\n",
    "clean_framingham = SpU.clean_df(framingham, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the features into a single vector column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------------------------------------------------+\n",
      "|TenYearCHD|features                                                               |\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "|0.0       |(15,[0,1,2,9,10,11,13,14],[1.0,39.0,4.0,195.0,106.0,70.0,80.0,77.0])   |\n",
      "|0.0       |(15,[1,2,9,10,11,13,14],[46.0,2.0,250.0,121.0,81.0,95.0,76.0])         |\n",
      "|0.0       |[1.0,48.0,1.0,1.0,20.0,0.0,0.0,0.0,0.0,245.0,0.0,80.0,0.0,75.0,70.0]   |\n",
      "|1.0       |[0.0,61.0,3.0,1.0,30.0,0.0,0.0,1.0,0.0,225.0,150.0,95.0,0.0,65.0,103.0]|\n",
      "|0.0       |[0.0,46.0,3.0,1.0,23.0,0.0,0.0,0.0,0.0,285.0,130.0,84.0,0.0,85.0,85.0] |\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = [\"male\", \"age\", \"education\", \"currentSmoker\", \"cigsPerDay\",\n",
    "                  \"BPMeds\", \"prevalentStroke\", \"prevalentHyp\", \"diabetes\",\n",
    "                  \"totChol\", \"sysBP\", \"diaBP\", \"BMI\", \"heartRate\", \"glucose\"]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "data_with_features = assembler.transform(clean_framingham).select(\"TenYearCHD\", \"features\")\n",
    "\n",
    "data_with_features.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets 80% training data and 20% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = data_with_features.randomSplit([0.8, 0.2], seed=57)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "|TenYearCHD|features                                                               |\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "|0.0       |(15,[0,1,2,9,10,11,13,14],[1.0,39.0,4.0,195.0,106.0,70.0,80.0,77.0])   |\n",
      "|0.0       |(15,[1,2,9,10,11,13,14],[46.0,2.0,250.0,121.0,81.0,95.0,76.0])         |\n",
      "|0.0       |[1.0,48.0,1.0,1.0,20.0,0.0,0.0,0.0,0.0,245.0,0.0,80.0,0.0,75.0,70.0]   |\n",
      "|1.0       |[0.0,61.0,3.0,1.0,30.0,0.0,0.0,1.0,0.0,225.0,150.0,95.0,0.0,65.0,103.0]|\n",
      "|0.0       |[0.0,46.0,3.0,1.0,23.0,0.0,0.0,0.0,0.0,285.0,130.0,84.0,0.0,85.0,85.0] |\n",
      "+----------+-----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Trained Dataset\n",
      "+----------+-----------------------------------------------------------------+\n",
      "|TenYearCHD|features                                                         |\n",
      "+----------+-----------------------------------------------------------------+\n",
      "|0.0       |(15,[0,1,2,3,4,7,9,13],[1.0,35.0,1.0,1.0,40.0,1.0,265.0,102.0])  |\n",
      "|0.0       |(15,[0,1,2,3,4,7,9,13],[1.0,48.0,2.0,1.0,43.0,1.0,210.0,95.0])   |\n",
      "|0.0       |(15,[0,1,2,3,4,7,9,13],[1.0,56.0,1.0,1.0,43.0,1.0,240.0,80.0])   |\n",
      "|0.0       |(15,[0,1,2,3,4,7,11,13],[1.0,50.0,3.0,1.0,30.0,1.0,105.0,72.0])  |\n",
      "|0.0       |(15,[0,1,2,3,4,9,10,13],[1.0,43.0,2.0,1.0,20.0,153.0,130.0,63.0])|\n",
      "+----------+-----------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original Dataset\")\n",
    "data_with_features.show(5, truncate=False)\n",
    "\n",
    "# Print train dataset\n",
    "print(\"Trained Dataset\")\n",
    "train_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, featuresCol=\"features\", labelCol=\"TenYearCHD\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.44442997924738903,0.06437115942304895,-0.01714871123433243,0.08886663411378709,0.01858201585886688,0.38586930633634703,0.6647169872764429,0.6242157540959542,0.6658870920934002,0.001214638645979052,0.00014111070591105773,0.004235863452192114,-0.0064485174402852926,0.0002774134475905535,0.0033566675802193364]\n"
     ]
    }
   ],
   "source": [
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Print coefficients\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "\n",
    "# Display model summary\n",
    "training_summary = lr_model.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions\n",
      "+----------------------------------------------------------------+----------+-----------------------------------------+\n",
      "|features                                                        |prediction|probability                              |\n",
      "+----------------------------------------------------------------+----------+-----------------------------------------+\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,39.0,2.0,1.0,20.0,212.0,87.0,77.0])|0.0       |[0.9360122009015007,0.06398779909849928] |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,39.0,2.0,1.0,20.0,222.0,73.0,64.0])|0.0       |[0.9380984781466017,0.061901521853398256]|\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,43.0,1.0,1.0,20.0,206.0,86.0,89.0])|0.0       |[0.9149467215917397,0.08505327840826027] |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,47.0,3.0,1.0,25.0,173.0,68.0,75.0])|0.0       |[0.8958145134146775,0.10418548658532245] |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,54.0,2.0,1.0,30.0,215.0,60.0,75.0])|0.0       |[0.823770974567436,0.17622902543256402]  |\n",
      "+----------------------------------------------------------------+----------+-----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "TenYearCHD\n",
      "+----------------------------------------------------------------+----------+\n",
      "|features                                                        |TenYearCHD|\n",
      "+----------------------------------------------------------------+----------+\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,39.0,2.0,1.0,20.0,212.0,87.0,77.0])|0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,39.0,2.0,1.0,20.0,222.0,73.0,64.0])|0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,43.0,1.0,1.0,20.0,206.0,86.0,89.0])|0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,47.0,3.0,1.0,25.0,173.0,68.0,75.0])|0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,54.0,2.0,1.0,30.0,215.0,60.0,75.0])|0.0       |\n",
      "+----------------------------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the trained model to make predictions on the test data\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Show predictions\n",
    "print(\"Predictions\")\n",
    "predictions.select(\"features\", \"prediction\", \"probability\").show(5, truncate=False)\n",
    "\n",
    "print(\"TenYearCHD\")\n",
    "predictions.select(\"features\", \"TenYearCHD\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------+----------+----------+\n",
      "|features                                                               |TenYearCHD|prediction|\n",
      "+-----------------------------------------------------------------------+----------+----------+\n",
      "|[1.0,52.0,3.0,1.0,35.0,0.0,0.0,1.0,1.0,281.0,133.0,93.0,0.0,115.0,80.0]|0.0       |1.0       |\n",
      "|[1.0,53.0,1.0,1.0,10.0,0.0,0.0,1.0,1.0,229.0,0.0,82.0,0.0,60.0,172.0]  |0.0       |1.0       |\n",
      "|[1.0,58.0,2.0,1.0,60.0,0.0,0.0,1.0,0.0,250.0,150.0,97.0,32.0,75.0,65.0]|0.0       |1.0       |\n",
      "|[1.0,66.0,1.0,1.0,20.0,0.0,0.0,1.0,0.0,228.0,188.0,128.0,0.0,84.0,67.0]|0.0       |1.0       |\n",
      "|(15,[0,1,2,8,9,13,14],[1.0,62.0,3.0,1.0,346.0,80.0,394.0])             |1.0       |1.0       |\n",
      "|[0.0,67.0,2.0,0.0,0.0,1.0,0.0,1.0,1.0,303.0,204.0,96.0,0.0,75.0,394.0] |1.0       |1.0       |\n",
      "|[1.0,62.0,1.0,1.0,23.0,0.0,0.0,1.0,0.0,286.0,164.0,88.0,0.0,85.0,126.0]|1.0       |1.0       |\n",
      "|[1.0,63.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,260.0,0.0,98.0,0.0,67.0,109.0]   |1.0       |1.0       |\n",
      "|[1.0,63.0,1.0,1.0,20.0,0.0,1.0,1.0,0.0,213.0,163.0,94.0,0.0,76.0,69.0] |1.0       |1.0       |\n",
      "|[1.0,63.0,1.0,1.0,30.0,0.0,0.0,1.0,0.0,225.0,146.0,82.0,0.0,70.0,85.0] |1.0       |1.0       |\n",
      "+-----------------------------------------------------------------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------------------------------------------------------------+----------+----------+\n",
      "|features                                                          |TenYearCHD|prediction|\n",
      "+------------------------------------------------------------------+----------+----------+\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,39.0,2.0,1.0,20.0,212.0,87.0,77.0])  |0.0       |0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,39.0,2.0,1.0,20.0,222.0,73.0,64.0])  |0.0       |0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,43.0,1.0,1.0,20.0,206.0,86.0,89.0])  |0.0       |0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,47.0,3.0,1.0,25.0,173.0,68.0,75.0])  |0.0       |0.0       |\n",
      "|(15,[0,1,2,3,4,9,13,14],[1.0,54.0,2.0,1.0,30.0,215.0,60.0,75.0])  |0.0       |0.0       |\n",
      "|(15,[0,1,2,7,9,10,13],[1.0,59.0,2.0,1.0,242.0,144.0,72.0])        |0.0       |0.0       |\n",
      "|(15,[0,1,2,7,9,10,13,14],[1.0,46.0,3.0,1.0,300.0,146.0,60.0,79.0])|0.0       |0.0       |\n",
      "|(15,[0,1,2,7,9,10,13,14],[1.0,48.0,1.0,1.0,245.0,144.0,75.0,77.0])|0.0       |0.0       |\n",
      "|(15,[0,1,2,7,9,10,13,14],[1.0,63.0,1.0,1.0,248.0,130.0,98.0,83.0])|0.0       |0.0       |\n",
      "|(15,[0,1,2,7,9,11,13],[1.0,42.0,4.0,1.0,245.0,85.0,62.0])         |0.0       |0.0       |\n",
      "+------------------------------------------------------------------+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show predictions and real values where prediction is 1\n",
    "predictions.filter(predictions.prediction == 1).select(\"features\", \"TenYearCHD\", \"prediction\").show(10, truncate=False)\n",
    "\n",
    "# Show predictions and real values where prediction is 0\n",
    "predictions.filter(predictions.prediction == 0).select(\"features\", \"TenYearCHD\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "label does not exist. Available: TenYearCHD, features, rawPrediction, probability, prediction",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassClassificationEvaluator\n\u001b[1;32m      3\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m MulticlassClassificationEvaluator(labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, predictionCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTenYearCHD\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetricName\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m precision \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions,{evaluator\u001b[38;5;241m.\u001b[39mmetricName: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweightedPrecision\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      9\u001b[0m recall \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions,{evaluator\u001b[38;5;241m.\u001b[39mmetricName: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweightedRecall\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/ml/evaluation.py:109\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/ml/evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: label does not exist. Available: TenYearCHD, features, rawPrediction, probability, prediction"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"TenYearCHD\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "precision = evaluator.evaluate(predictions,{evaluator.metricName: \"weightedPrecision\"})\n",
    "\n",
    "recall = evaluator.evaluate(predictions,{evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "f1 = evaluator.evaluate(predictions,{evaluator.metricName: \"f1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/27 18:58:38 WARN JavaUtils: Attempt to delete using native Unix OS command failed for path = /tmp/spark-c9367a43-5ec1-4dfc-bd39-37c4c5079670. Falling back to Java IO way\n",
      "java.io.IOException: Failed to delete: /tmp/spark-c9367a43-5ec1-4dfc-bd39-37c4c5079670\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingUnixNative(JavaUtils.java:174)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:109)\n",
      "\tat org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:90)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively(SparkFileUtils.scala:121)\n",
      "\tat org.apache.spark.util.SparkFileUtils.deleteRecursively$(SparkFileUtils.scala:120)\n",
      "\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1126)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)\n",
      "\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1328)\n",
      "\tat org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat scala.util.Try$.apply(Try.scala:210)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
