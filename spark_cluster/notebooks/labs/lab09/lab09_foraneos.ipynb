{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Lab 09 - Watermarking with Spark** </center>\n",
    "---\n",
    "## <center> **Big Data** </center>\n",
    "---\n",
    "### <center> **Spring 2025** </center>\n",
    "---\n",
    "### <center> **04/09/2025** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "\n",
    "**Team**: Foraneos\n",
    "\n",
    "**Students**: Eddie, Konrad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark Session creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "konrad_port = \"0638c7435d1d\"\n",
    "eddie_port = \"8776010e8f6a\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkSQLStructuredStreaming-Kafka\") \\\n",
    "    .master(\"spark://{}:7077\".format(eddie_port)) \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.4\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kafka Stream init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka_lines = spark \\\n",
    "                .readStream \\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\", \"7f12b05dfecd:9093\") \\\n",
    "                .option(\"subscribe\", \"kafka-spark-example\") \\\n",
    "                .load()\n",
    "\n",
    "kafka_lines.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform binary data into string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = kafka_lines.withColumn(\"value_str\", kafka_lines.value.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = false)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "words = kafka_df.select(explode(split(kafka_df.value, \" \")).alias(\"word\"), \"timestamp\")\n",
    "words.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Watermarking to handle late arrival events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`word`, `window`, `timestamp`].;\n'Aggregate [window#277, word#265], [window#277 AS window#268, word#265, min('value) AS min(value)#274, avg('value) AS avg(value)#275, max('value) AS max(value)#276]\n+- Filter isnotnull(timestamp#246)\n   +- Expand [[named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 0) + 60000000), LongType, TimestampType))), word#265, timestamp#246-T120000ms], [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 30000000), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 30000000) + 60000000), LongType, TimestampType))), word#265, timestamp#246-T120000ms]], [window#277, word#265, timestamp#246-T120000ms]\n      +- EventTimeWatermark timestamp#246: timestamp, 2 minutes\n         +- Project [word#265, timestamp#246]\n            +- Generate explode(split(cast(value#242 as string),  , -1)), false, [word#265]\n               +- Project [key#241, value#242, topic#243, partition#244, offset#245L, timestamp#246, timestampType#247, cast(value#242 as string) AS value_str#255]\n                  +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@ba3eba2, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4667e244, [kafka.bootstrap.servers=7f12b05dfecd:9093, subscribe=kafka-spark-example], [key#241, value#242, topic#243, partition#244, offset#245L, timestamp#246, timestampType#247], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38cbdf53,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 7f12b05dfecd:9093, subscribe -> kafka-spark-example),None), kafka, [key#234, value#235, topic#236, partition#237, offset#238L, timestamp#239, timestampType#240]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m window, avg, \u001b[38;5;28mmin\u001b[39m, \u001b[38;5;28mmax\u001b[39m\n\u001b[1;32m      2\u001b[0m windowed_counts \u001b[38;5;241m=\u001b[39m  \u001b[43mwords\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithWatermark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2 minutes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m60 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Window duration \u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m30 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Slide duration\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 8\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;241m8\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/sql/group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/spark-3.5.4-bin-hadoop3-scala2.13/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `value` cannot be resolved. Did you mean one of the following? [`word`, `window`, `timestamp`].;\n'Aggregate [window#277, word#265], [window#277 AS window#268, word#265, min('value) AS min(value)#274, avg('value) AS avg(value)#275, max('value) AS max(value)#276]\n+- Filter isnotnull(timestamp#246)\n   +- Expand [[named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 0), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 0) + 60000000), LongType, TimestampType))), word#265, timestamp#246-T120000ms], [named_struct(start, knownnullable(precisetimestampconversion(((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 30000000), LongType, TimestampType)), end, knownnullable(precisetimestampconversion((((precisetimestampconversion(timestamp#246, TimestampType, LongType) - CASE WHEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) < cast(0 as bigint)) THEN (((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) + 30000000) ELSE ((precisetimestampconversion(timestamp#246, TimestampType, LongType) - 0) % 30000000) END) - 30000000) + 60000000), LongType, TimestampType))), word#265, timestamp#246-T120000ms]], [window#277, word#265, timestamp#246-T120000ms]\n      +- EventTimeWatermark timestamp#246: timestamp, 2 minutes\n         +- Project [word#265, timestamp#246]\n            +- Generate explode(split(cast(value#242 as string),  , -1)), false, [word#265]\n               +- Project [key#241, value#242, topic#243, partition#244, offset#245L, timestamp#246, timestampType#247, cast(value#242 as string) AS value_str#255]\n                  +- StreamingRelationV2 org.apache.spark.sql.kafka010.KafkaSourceProvider@ba3eba2, kafka, org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@4667e244, [kafka.bootstrap.servers=7f12b05dfecd:9093, subscribe=kafka-spark-example], [key#241, value#242, topic#243, partition#244, offset#245L, timestamp#246, timestampType#247], StreamingRelation DataSource(org.apache.spark.sql.SparkSession@38cbdf53,kafka,List(),None,List(),None,Map(kafka.bootstrap.servers -> 7f12b05dfecd:9093, subscribe -> kafka-spark-example),None), kafka, [key#234, value#235, topic#236, partition#237, offset#238L, timestamp#239, timestampType#240]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, avg, min, max\n",
    "windowed_counts =  words \\\n",
    "                        .withWatermark(\"timestamp\", \"2 minutes\") \\\n",
    "                        .groupBy(window(words.timestamp, \n",
    "                                        \"60 seconds\", # Window duration \n",
    "                                        \"30 seconds\"), # Slide duration\n",
    "                                 words.word) \\\n",
    "                        .agg(min(\"value\"), avg(\"value\"), max(\"value\"))\n",
    "8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sink configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/10 05:40:11 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-de5768eb-2706-47cb-9c5a-90edd10f7702. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/04/10 05:40:11 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "25/04/10 05:40:11 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----+-----+\n",
      "|window                                    |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2025-04-10 05:40:00, 2025-04-10 05:41:00}|41  |1    |\n",
      "|{2025-04-10 05:40:00, 2025-04-10 05:41:00}|532 |1    |\n",
      "|{2025-04-10 05:40:00, 2025-04-10 05:41:00}|63  |3    |\n",
      "|{2025-04-10 05:40:30, 2025-04-10 05:41:30}|42  |1    |\n",
      "|{2025-04-10 05:40:00, 2025-04-10 05:41:00}|42  |1    |\n",
      "|{2025-04-10 05:40:30, 2025-04-10 05:41:30}|532 |1    |\n",
      "|{2025-04-10 05:40:30, 2025-04-10 05:41:30}|63  |3    |\n",
      "|{2025-04-10 05:40:30, 2025-04-10 05:41:30}|41  |1    |\n",
      "+------------------------------------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----+-----+\n",
      "|window                                    |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|6   |1    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|4   |1    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|5   |2    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|53  |1    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|2   |2    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|2   |2    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|23  |2    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|45  |2    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|6   |1    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|43  |1    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|5   |2    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|53  |1    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|45  |2    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|43  |1    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|23  |2    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|4   |1    |\n",
      "|{2025-04-10 05:41:30, 2025-04-10 05:42:30}|35  |1    |\n",
      "|{2025-04-10 05:41:00, 2025-04-10 05:42:00}|35  |1    |\n",
      "+------------------------------------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+------------------------------------------+----+-----+\n",
      "|window                                    |word|count|\n",
      "+------------------------------------------+----+-----+\n",
      "|{2025-04-10 05:42:30, 2025-04-10 05:43:30}|345 |1    |\n",
      "|{2025-04-10 05:42:30, 2025-04-10 05:43:30}|34  |1    |\n",
      "|{2025-04-10 05:42:30, 2025-04-10 05:43:30}|35  |1    |\n",
      "|{2025-04-10 05:43:00, 2025-04-10 05:44:00}|3   |1    |\n",
      "|{2025-04-10 05:42:30, 2025-04-10 05:43:30}|45  |1    |\n",
      "|{2025-04-10 05:43:00, 2025-04-10 05:44:00}|35  |1    |\n",
      "|{2025-04-10 05:42:30, 2025-04-10 05:43:30}|53  |2    |\n",
      "|{2025-04-10 05:42:30, 2025-04-10 05:43:30}|3   |1    |\n",
      "|{2025-04-10 05:43:00, 2025-04-10 05:44:00}|345 |1    |\n",
      "|{2025-04-10 05:43:00, 2025-04-10 05:44:00}|34  |1    |\n",
      "|{2025-04-10 05:43:00, 2025-04-10 05:44:00}|45  |1    |\n",
      "|{2025-04-10 05:43:00, 2025-04-10 05:44:00}|53  |2    |\n",
      "+------------------------------------------+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+------+----+-----+\n",
      "|window|word|count|\n",
      "+------+----+-----+\n",
      "+------+----+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "query = windowed_counts \\\n",
    "                .writeStream \\\n",
    "                .outputMode(\"update\") \\\n",
    "                .trigger(processingTime='30 seconds') \\\n",
    "                .format(\"console\") \\\n",
    "                .option(\"truncate\", \"false\") \\\n",
    "                .start()\n",
    "\n",
    "query.awaitTermination(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
