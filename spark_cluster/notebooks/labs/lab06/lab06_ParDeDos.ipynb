{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# <center> <img src=\"../img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
        "# <center> **Departamento de Electrónica, Sistemas e Informática** </center>\n",
        "---\n",
        "## <center> Program: _Computer Systems Engineering_  </center>\n",
        "---\n",
        "### <center> **Spring 2025** </center>\n",
        "---\n",
        "\n",
        "**Lab 06**: Big Data Pipeline for Netflix data.\n",
        "\n",
        "**Date**: 10/03/2025\n",
        "\n",
        "**Team Name**: Par De Dos\n",
        "\n",
        "**Professor**: Pablo Camarillo Ramirez"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize findspark to get acces to de PySpark installation\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "25/03/11 01:37:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create connection to the spark cluster\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Lab 05 - Par de Dos\") \\\n",
        "    .master(\"spark://7a359a1e9836:7077\") \\\n",
        "    .config(\"spark.ui.port\",\"4040\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create SparkContext\n",
        "sc = spark.sparkContext\n",
        "sc.setLogLevel(\"ERROR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from team_ParDeDos.spark_utils import SparkUtils\n",
        "netflix_schema = SparkUtils.generate_schema([\n",
        "\t(\"show_id\", \"string\"),\n",
        "\t(\"type\", \"string\"),\n",
        "\t(\"title\", \"string\"),\n",
        "\t(\"director\", \"string\"),\n",
        "\t(\"country\", \"string\"),\n",
        "\t(\"rating\", \"string\"),\n",
        "\t(\"duration\", \"string\"),\n",
        "\t(\"listed_in\", \"string\"),\n",
        "\t(\"release_year\", \"integer\"),\n",
        "\t(\"date_added\", \"date\")\n",
        "])\n",
        "\n",
        "netflix_df = spark.read.schema(netflix_schema).option(\"header\", \"true\").csv(\"/home/jovyan/notebooks/data/netflix1.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- show_id: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            " |-- title: string (nullable = true)\n",
            " |-- director: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- rating: string (nullable = true)\n",
            " |-- duration: string (nullable = true)\n",
            " |-- listed_in: string (nullable = true)\n",
            " |-- release_year: integer (nullable = true)\n",
            " |-- date_added: date (nullable = true)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-------+--------------------------------+---------------+-------------+---------+--------+---------+------------+----------+\n",
            "|show_id|type   |title                           |director       |country      |rating   |duration|listed_in|release_year|date_added|\n",
            "+-------+-------+--------------------------------+---------------+-------------+---------+--------+---------+------------+----------+\n",
            "|s1     |Movie  |Dick Johnson Is Dead            |Kirsten Johnson|United States|9/25/2021|2020    |PG-13    |NULL        |NULL      |\n",
            "|s3     |TV Show|Ganglands                       |Julien Leclercq|France       |9/24/2021|2021    |TV-MA    |NULL        |NULL      |\n",
            "|s6     |TV Show|Midnight Mass                   |Mike Flanagan  |United States|9/24/2021|2021    |TV-MA    |NULL        |NULL      |\n",
            "|s14    |Movie  |Confessions of an Invisible Girl|Bruno Garotti  |Brazil       |9/22/2021|2021    |TV-PG    |NULL        |NULL      |\n",
            "|s8     |Movie  |Sankofa                         |Haile Gerima   |United States|9/24/2021|1993    |TV-MA    |NULL        |NULL      |\n",
            "+-------+-------+--------------------------------+---------------+-------------+---------+--------+---------+------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "netflix_df.printSchema()\n",
        "netflix_df.show(5, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable StructField object",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m netflix_cleaned \u001b[38;5;241m=\u001b[39m \u001b[43mSparkUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclean_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetflix_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnetflix_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m netflix_cleaned\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[1;32m      4\u001b[0m netflix_cleaned\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m~/notebooks/lib/team_ParDeDos/spark_utils.py:42\u001b[0m, in \u001b[0;36mSparkUtils.clean_df\u001b[0;34m(df, schema)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m(\"show_id\", \"string\"),\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m(\"type\", \"string\"),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m(\"date_added\", \"date\")\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     28\u001b[0m defaults \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhola\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minteger\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m15\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlol\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m }\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m schema:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m defaults:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable StructField object"
          ]
        }
      ],
      "source": [
        "netflix_cleaned = SparkUtils.clean_df(netflix_df, netflix_schema)\n",
        "\n",
        "netflix_cleaned.printSchema()\n",
        "netflix_cleaned.show(5, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'sc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Stop the SparkContext\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msc\u001b[49m\u001b[38;5;241m.\u001b[39mstop()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sc' is not defined"
          ]
        }
      ],
      "source": [
        "# Stop the SparkContext\n",
        "sc.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
