{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> <img src=\"../labs/img/ITESOLogo.png\" alt=\"ITESO\" width=\"480\" height=\"130\"> </center>\n",
    "# <center> **Departamento de Electr칩nica, Sistemas e Inform치tica** </center>\n",
    "---\n",
    "## <center> **Procesamiento de Datos Masivos** </center>\n",
    "---\n",
    "### <center> **Primavera 2025** </center>\n",
    "---\n",
    "### <center> **lab12** </center>\n",
    "\n",
    "---\n",
    "**Profesor**: Dr. Pablo Camarillo Ramirez\n",
    "\n",
    "**Team name**: Par de Dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creacion de la conexi칩n con el cluster de spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/11 19:48:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MLSpark-Recommender-Systems\") \\\n",
    "    .master(\"spark://66728873c271:7077\") \\\n",
    "    .config(\"spark.ui.port\",\"4040\") \\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparaci칩n de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+\n",
      "|user_id|movie_id|rating| timestamp|\n",
      "+-------+--------+------+----------+\n",
      "|      0|       2|     3|1424380312|\n",
      "|      0|       3|     1|1424380312|\n",
      "|      0|       5|     2|1424380312|\n",
      "|      0|       9|     4|1424380312|\n",
      "|      0|      11|     1|1424380312|\n",
      "|      0|      12|     2|1424380312|\n",
      "|      0|      15|     1|1424380312|\n",
      "|      0|      17|     1|1424380312|\n",
      "|      0|      19|     1|1424380312|\n",
      "|      0|      21|     1|1424380312|\n",
      "|      0|      23|     1|1424380312|\n",
      "|      0|      26|     3|1424380312|\n",
      "|      0|      27|     1|1424380312|\n",
      "|      0|      28|     1|1424380312|\n",
      "|      0|      29|     1|1424380312|\n",
      "|      0|      30|     1|1424380312|\n",
      "|      0|      31|     1|1424380312|\n",
      "|      0|      34|     1|1424380312|\n",
      "|      0|      37|     1|1424380312|\n",
      "|      0|      41|     2|1424380312|\n",
      "+-------+--------+------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- movie_id: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from team_name.spark_utils import SparkUtils\n",
    "from team_ParDeDos.spark_utils import SparkUtils as SpU\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "schema = SpU.generate_schema([\n",
    "        (\"user_id\", \"integer\"),\n",
    "        (\"song_id\", \"integer\"),\n",
    "        (\"rating\", \"integer\"),\n",
    "        (\"timestamp\", \"long\")\n",
    "    ])\n",
    "\n",
    "data = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .text(\"/home/jovyan/notebooks/data/sample_movielens_ratings.txt\")\n",
    "\n",
    "data = data.select(\n",
    "    split(col(\"value\"), \"::\").getItem(0).cast(\"integer\").alias(\"user_id\"),\n",
    "    split(col(\"value\"), \"::\").getItem(1).cast(\"integer\").alias(\"movie_id\"),\n",
    "    split(col(\"value\"), \"::\").getItem(2).cast(\"integer\").alias(\"rating\"),\n",
    "    split(col(\"value\"), \"::\").getItem(3).cast(\"long\").alias(\"timestamp\")\n",
    ")\n",
    "\n",
    "data.show()\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure ALS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(\n",
    "    userCol=\"user_id\",\n",
    "    itemCol=\"movie_id\",\n",
    "    ratingCol=\"rating\",\n",
    "    maxIter=10,\n",
    "    regParam=0.1,\n",
    "    rank=5,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = als.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:====================================================>  (96 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------------------------------------------------------+\n",
      "|user_id|recommendations                                                                      |\n",
      "+-------+-------------------------------------------------------------------------------------+\n",
      "|0      |[{92, 2.5840385}, {2, 2.316802}, {62, 2.2325232}, {25, 2.157748}, {93, 2.1528697}]   |\n",
      "|10     |[{92, 2.768342}, {2, 2.6728113}, {93, 2.6242015}, {25, 2.5927775}, {49, 2.5867324}]  |\n",
      "|20     |[{22, 3.5597918}, {68, 3.1278815}, {94, 3.084497}, {51, 3.0827737}, {77, 3.0246763}] |\n",
      "|1      |[{22, 2.9029422}, {68, 2.630123}, {77, 2.5238972}, {62, 2.501064}, {90, 2.4797387}]  |\n",
      "|11     |[{32, 5.082464}, {18, 4.705235}, {30, 4.6826043}, {27, 4.5120797}, {8, 4.229401}]    |\n",
      "|21     |[{29, 4.320379}, {52, 4.2401457}, {76, 3.716108}, {63, 3.5063725}, {53, 3.4859684}]  |\n",
      "|22     |[{51, 4.458179}, {75, 4.418395}, {22, 4.118836}, {74, 4.1007586}, {88, 4.0829244}]   |\n",
      "|2      |[{93, 4.2531066}, {83, 4.1469526}, {8, 4.0344357}, {39, 3.7214603}, {2, 3.6790943}]  |\n",
      "|12     |[{46, 5.763551}, {55, 4.7998195}, {49, 4.545951}, {90, 4.2674875}, {48, 4.1170616}]  |\n",
      "|23     |[{46, 5.4916263}, {55, 4.7129517}, {32, 4.655011}, {90, 4.6475368}, {49, 4.432613}]  |\n",
      "|3      |[{30, 4.089861}, {69, 3.8413658}, {51, 3.807304}, {74, 3.7822077}, {75, 3.7480934}]  |\n",
      "|13     |[{74, 2.7260454}, {93, 2.616767}, {29, 2.5265772}, {53, 2.495797}, {52, 2.447867}]   |\n",
      "|24     |[{52, 4.4483128}, {29, 4.445633}, {30, 3.9009523}, {53, 3.8602533}, {69, 3.7498972}] |\n",
      "|4      |[{29, 3.2479038}, {52, 3.1375268}, {62, 2.8016934}, {93, 2.767445}, {76, 2.758533}]  |\n",
      "|14     |[{29, 4.6415415}, {52, 4.596407}, {76, 3.866494}, {63, 3.81995}, {85, 3.5510547}]    |\n",
      "|5      |[{46, 4.169175}, {90, 3.561502}, {55, 3.322591}, {49, 3.2502005}, {94, 3.189144}]    |\n",
      "|15     |[{46, 3.4862611}, {49, 2.7943614}, {55, 2.670787}, {90, 2.6077518}, {64, 2.45374}]   |\n",
      "|25     |[{46, 3.1664221}, {49, 2.9964566}, {55, 2.5469325}, {93, 2.3547156}, {48, 2.3131235}]|\n",
      "|6      |[{29, 3.1030846}, {52, 3.0379107}, {25, 3.02641}, {2, 2.7636726}, {93, 2.7427537}]   |\n",
      "|16     |[{22, 3.7481875}, {77, 3.6971643}, {90, 3.6679487}, {85, 3.6157954}, {62, 3.5778081}]|\n",
      "+-------+-------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "user_recommendations = model.recommendForAllUsers(numItems=5)\n",
    "\n",
    "user_recommendations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+----------+----------+\n",
      "|user_id|movie_id|rating|timestamp |prediction|\n",
      "+-------+--------+------+----------+----------+\n",
      "|22     |0       |1     |1424380312|0.96697557|\n",
      "|22     |3       |2     |1424380312|1.6326257 |\n",
      "|22     |5       |2     |1424380312|2.0366673 |\n",
      "|22     |6       |2     |1424380312|2.2972772 |\n",
      "|22     |9       |1     |1424380312|1.5513803 |\n",
      "|22     |10      |1     |1424380312|1.4349127 |\n",
      "|22     |11      |1     |1424380312|1.2901659 |\n",
      "|22     |13      |1     |1424380312|1.617328  |\n",
      "|22     |14      |1     |1424380312|1.389045  |\n",
      "|22     |16      |1     |1424380312|0.7093756 |\n",
      "|22     |18      |3     |1424380312|3.0116072 |\n",
      "|22     |19      |1     |1424380312|1.4644071 |\n",
      "|22     |22      |5     |1424380312|4.118836  |\n",
      "|22     |25      |1     |1424380312|0.97840077|\n",
      "|22     |26      |1     |1424380312|1.1323681 |\n",
      "|22     |29      |3     |1424380312|3.2431226 |\n",
      "|22     |30      |5     |1424380312|3.9942718 |\n",
      "|22     |32      |4     |1424380312|3.217519  |\n",
      "|22     |33      |1     |1424380312|0.8903809 |\n",
      "|22     |35      |1     |1424380312|0.7503733 |\n",
      "+-------+--------+------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(data)\n",
    "predictions.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error (RMSE) = 0.5691166521341573\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root-mean-square error (RMSE) = {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
